import asyncio
import aiohttp
from bs4 import BeautifulSoup
from bs4.element import Tag
from datetime import datetime, timedelta
import re
import nest_asyncio


nest_asyncio.apply()


def convert_time_format(time_string):
    """
    å°‡ "Sun Jul 27 13:45:00 2025" æ ¼å¼çš„æ™‚é–“å­—ä¸²è½‰æ›ç‚º "YYYY-MM-DD" æ ¼å¼ã€‚

    Args:
        time_string (str): åŸå§‹æ™‚é–“å­—ä¸²ï¼Œä¾‹å¦‚ "Sun Jul 27 13:45:00 2025"ã€‚

    Returns:
        str: è½‰æ›å¾Œçš„ "YYYY-MM-DD" æ ¼å¼æ™‚é–“å­—ä¸²ï¼Œä¾‹å¦‚ "2025-07-27"ã€‚
    """
    # å®šç¾©åŸå§‹æ™‚é–“å­—ä¸²çš„æ ¼å¼
    # %a: æ˜ŸæœŸç¸®å¯« (Sun)
    # %b: æœˆä»½ç¸®å¯« (Jul)
    # %d: æœˆä»½ä¸­çš„æ—¥æœŸ (27)
    # %H: 24 å°æ™‚åˆ¶å°æ™‚ (13)
    # %M: åˆ†é˜ (45)
    # %S: ç§’ (00)
    # %Y: å››ä½æ•¸å¹´ä»½ (2025)
    original_format = "%a %b %d %H:%M:%S %Y"

    # å°‡å­—ä¸²è§£æç‚º datetime ç‰©ä»¶
    dt_object = datetime.strptime(time_string, original_format)

    # å°‡ datetime ç‰©ä»¶æ ¼å¼åŒ–ç‚ºç›®æ¨™å­—ä¸²æ ¼å¼
    target_format = "%Y-%m-%d"
    return dt_object.strftime(target_format)

# ========= å¯ä¾éœ€æ±‚ä¿®æ”¹çš„åƒæ•¸ =========
PTT_URL = 'https://www.ptt.cc'
PAGES_TO_CHECK = 10         # æœ€å¤šå¾€å‰ç¿»æŸ¥çš„é æ•¸
PUSH_LIMIT = 20            # æ¨æ–‡æ•¸é–€æª»ï¼ˆçˆ† or â‰¥ æ•¸å­—ï¼‰
DAYS_LIMIT = 7              # åªæŠ“æœ€è¿‘ N å¤©çš„æ–‡ç« 
SLEEP_SECONDS = 0.1         # æ¯é æŠ“å®Œå¾Œå»¶é²ç§’æ•¸
MAX_POSTS_PER_BOARD = 5    # æ¯å€‹çœ‹æ¿æœ€å¤šæŠ“å¹¾ç¯‡æ–‡ç« 
PTT_BOARDS = ["Gossiping", "Military", "PublicIssue",
    "Stock", "Finance", "Bank_Service", "Tech_Job", 
    "Soft_Job", "Salary","Boy-Girl", "WomenTalk", 
    "LGBT_SEX", "MobileGame", "movie", "TW_Entertain",
    "MobileComm", "PC_Shopping", "nb-shopping", "HardwareSale",
    "home-sale", "NBA", "Baseball", "FITNESS", "Japan_Travel"
    ]  
"""
PTT_BOARDS = ["Gossiping", "Military", "PublicIssue",
    "Stock", "Finance", "Lifeismoney", "Bank_Service",
    "Tech_Job", "Soft_Job", "CV_help", "Salary",
    "Boy-Girl", "WomenTalk", "marriage", "LGBT_SEX",
    "C_Chat", "LoL", "MobileGame", "Steam", "ToS",
    "movie", "KoreaDrama", "Japandrama", "TW_Entertain",
    "MobileComm", "PC_Shopping", "nb-shopping", "HardwareSale",
    "home-sale", "Rent_tao", "home-appliance", "furnishing",
    "NBA", "Baseball", "SportLottery", "FITNESS",
    "car", "biker", "Railway", "Japan_Travel"
    ]  # é è¨­è¦çˆ¬çš„çœ‹æ¿æ¸…å–®
"""
headers = {"User-Agent": "Mozilla/5.0"}
DATE_LIMIT = datetime.today() - timedelta(days=DAYS_LIMIT)

# ========= æ¸…æ´—æ–‡ç« å…§å®¹ =========
def clean_content(raw_text: str) -> str:
    raw_text = raw_text.split('â€» ç™¼ä¿¡ç«™:')[0]
    lines = raw_text.splitlines()
    cleaned = []
    for line in lines:
        if re.match(r'^(ä½œè€…|çœ‹æ¿|æ¨™é¡Œ|æ™‚é–“)\s*:', line.strip()):
            continue
        cleaned.append(line)
    return re.sub(r'\n{3,}', '\n\n', "\n".join(cleaned)).strip()

# ========= æŠ“å–®ç¯‡æ–‡ç«  =========
async def fetch_post(session, href, visited_urls):
    url = PTT_URL + href
    if url in visited_urls:
        return None

    try:
        async with session.get(url, cookies={'over18': '1'}, headers=headers) as resp:
            if resp.status != 200:
                print('PTT,ç„¡è³‡æ–™')
                return None

            html = await resp.text()
            soup = BeautifulSoup(html, 'html.parser')
           

            date_str = ''
            for tag, val in zip(soup.select('span.article-meta-tag'), soup.select('span.article-meta-value')):
                if tag.text == 'æ™‚é–“':
                    date_str = val.text
                    try:
                        parsed_time = datetime.strptime(date_str, "%a %b %d %H:%M:%S %Y")
                        if parsed_time < DATE_LIMIT:
                            return None
                    except:
                        return None
                    break

            main_content = soup.find(id='main-content')
            if not main_content:
                return None

            # æ“·å–ç•™è¨€
            comments = []
            for push in main_content.find_all("div", class_="push"):
                try:
                    tag = push.find("span", class_="push-tag")
                    user = push.find("span", class_="push-userid")
                    content = push.find("span", class_="push-content")
                    if tag and user and content:
                        comments.append(f"{tag.text.strip()} {user.text.strip()}: {content.text.strip().lstrip(':')}")
                except:
                    continue

            # ç§»é™¤éç•™è¨€çš„ tag
            for tag in main_content.find_all(['div', 'span']):
                if not isinstance(tag, Tag):
                    continue
                try:
                    tag_class = tag.get('class') or []
                    if 'push' not in tag_class:
                        tag.decompose()
                except:
                    continue

            content_raw = main_content.get_text('\n').strip()
            content_cleaned = clean_content(content_raw)

            title_tag = soup.find('title')
            title = title_tag.text.strip() if title_tag else '(ç„¡æ¨™é¡Œ)'

            visited_urls.add(url)
            return {
                'title': title,
                'date': convert_time_format(date_str),
                'post_url': url,
                'summary': content_cleaned,
                'comments': comments, # è‹¥è¦æ”¹ç”¨ã€Œï½œã€åˆ†éš”å¯æ”¹æˆï¼š"ï½œ".join(comments)
                'source': 'PTT',
            }

    except:
        return None

# ========= æŠ“ä¸€å€‹çœ‹æ¿ =========
async def crawl_board(board_name, visited_urls):
    posts = []
    collected = 0
    page_index = ''

    async with aiohttp.ClientSession() as session:
        for _ in range(PAGES_TO_CHECK):
            page_url = f"{PTT_URL}/bbs/{board_name}/index{page_index}.html" if page_index else f"{PTT_URL}/bbs/{board_name}/index.html"

            try:
                async with session.get(page_url, cookies={'over18': '1'}, headers=headers) as resp:
                    if resp.status != 200:
                        continue
                    html = await resp.text()
            except:
                continue

            soup = BeautifulSoup(html, 'html.parser')
            if not soup:
                print('PTT,boardç„¡è³‡æ–™')
                continue
            tasks = []

            for entry in soup.select('div.r-ent'):
                try:
                    push_text = entry.select_one('div.nrec').text.strip()
                    is_hot = (push_text == 'çˆ†') or (push_text.isdigit() and int(push_text) >= PUSH_LIMIT)
                    if not is_hot:
                        continue

                    link_tag = entry.select_one('div.title a')
                    if not link_tag:
                        continue

                    href = link_tag['href']
                    tasks.append(fetch_post(session, href, visited_urls))

                    if len(tasks) >= MAX_POSTS_PER_BOARD - collected:
                        break
                except:
                    continue

            results = await asyncio.gather(*tasks)
            for post in results:
                if post:
                    posts.append(post)
                    collected += 1
                    if collected >= MAX_POSTS_PER_BOARD:
                        return posts

            # ä¸‹ä¸€é 
            prev_btns = soup.find_all('a', class_='btn wide')
            for btn in prev_btns:
                if 'ä¸Šé ' in btn.text:
                    m = re.search(r'index(\d+)\.html', btn['href'])
                    if m:
                        page_index = m.group(1)

            await asyncio.sleep(SLEEP_SECONDS)  # æ¯é ä¹‹é–“å»¶é²

    return posts

# ========= ä¸»çˆ¬èŸ²æµç¨‹ =========
async def _main():
    visited_urls = set()
    all_posts = []
    for board in PTT_BOARDS:
        posts = await crawl_board(board, visited_urls)
        all_posts.extend(posts)
    return all_posts

# ========= å¤–éƒ¨åŒ¯å…¥çš„ä¸»å‡½å¼ =========
def get_ptt_posts():
    return asyncio.run(_main())
def ptt_keyword(keyword,posts):
    ptt_post = []
    for post in posts:
        if keyword in post['title']:
            ptt_post.append(post)
        elif keyword in post['summary']:
            ptt_post.append(post)
        else:
            for comment in post['comments']:
                if keyword in comment:
                    ptt_post.append(post)
                    break
    return ptt_post

if __name__ == "__main__":
    posts = get_ptt_posts()
    print(f"\nğŸ¯ å…±ç²å– {len(posts)} ç¯‡æ–‡ç« ")
    if posts:
        for k, v in posts[0].items():
            print(f"{k}:\n{v}\n{'-'*40}")


