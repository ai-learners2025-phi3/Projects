from datetime import datetime, timedelta
import time, json, random
from dateutil import parser, tz
import re

from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException

import jieba.analyse
from selenium.common.exceptions import StaleElementReferenceException


def login_to_threads(driver):
    IG_USERNAME = "leafwann_"
    IG_PASSWORD = "threads2025"

    driver.get("https://www.threads.net/login")

    try:
        # å°‹æ‰¾ä¸¦ç­‰å¾…ã€Œä½¿ç”¨ Instagram å¸³è™Ÿç¹¼çºŒã€æŒ‰éˆ•è®Šå¾—å¯é»æ“Š
        login_button = WebDriverWait(driver, 15).until(
            EC.element_to_be_clickable((By.XPATH, "//span[text()='ä½¿ç”¨ Instagram å¸³è™Ÿç¹¼çºŒ']"))
        )
        print("âœ… æ‰¾åˆ°ç™»å…¥æŒ‰éˆ•ï¼Œé»æ“Šä¸­...")
        login_button.click()
    except TimeoutException:
        print("âŒ æ‰¾ä¸åˆ°ç™»å…¥æŒ‰éˆ•ï¼Œç­‰å¾…è¶…æ™‚ã€‚")
        return False
    except Exception as e:
        print(f"âŒ ç™»å…¥æŒ‰éˆ•é»æ“Šå¤±æ•—ï¼Œå¯èƒ½æ˜¯è¢«å…¶ä»–å…ƒç´ é˜»æ“‹: {e}")
        # åœ¨é€™è£¡åŠ å…¥ç­‰å¾…é˜»æ“‹å…ƒç´ æ¶ˆå¤±çš„é‚è¼¯
        try:
            print("â³ åµæ¸¬åˆ°é˜»æ“‹å…ƒç´ ï¼Œæ­£åœ¨ç­‰å¾…å…¶æ¶ˆå¤±...")
            WebDriverWait(driver, 10).until(
                EC.invisibility_of_element_located((By.XPATH, "//div[@role='alert']"))
            )
            # å¦‚æœå½ˆå‡ºè¦–çª—æ¶ˆå¤±ï¼Œé‡æ–°å˜—è©¦é»æ“Šç™»å…¥æŒ‰éˆ•
            login_button = WebDriverWait(driver, 5).until(
                EC.element_to_be_clickable((By.XPATH, "//span[text()='ä½¿ç”¨ Instagram å¸³è™Ÿç¹¼çºŒ']"))
            )
            login_button.click()
            print("âœ… é‡æ–°é»æ“Šç™»å…¥æŒ‰éˆ•æˆåŠŸï¼")
        except:
            print("âŒ ç„¡æ³•è™•ç†é˜»æ“‹å…ƒç´ ï¼Œç™»å…¥æµç¨‹å¤±æ•—ã€‚")
            return False

    try:
        # ç­‰å¾…ä½¿ç”¨è€…åç¨±è¼¸å…¥æ¡†å‡ºç¾
        WebDriverWait(driver, 15).until(EC.presence_of_element_located((By.NAME, "username")))
        print("âœ… æˆåŠŸé€²å…¥ IG ç™»å…¥é é¢ï¼Œè¼¸å…¥å¸³å¯†ä¸­...")
        driver.find_element(By.NAME, "username").send_keys(IG_USERNAME)
        driver.find_element(By.NAME, "password").send_keys(IG_PASSWORD)
        driver.find_element(By.NAME, "password").send_keys(Keys.ENTER)
        
        # ç­‰å¾…ç™»å…¥æˆåŠŸå¾Œçš„é é¢å…ƒç´ 
        WebDriverWait(driver, 20).until(
            EC.presence_of_element_located((By.XPATH, "//div[@data-pressable-container='true']"))
        )
        print("âœ… æˆåŠŸç™»å…¥ Threadsã€‚")
        return True
    except TimeoutException:
        print("âŒ ç™»å…¥æµç¨‹è¶…æ™‚ï¼Œç™»å…¥å¤±æ•—ã€‚")
        return False
    except Exception as e:
        print(f"âŒ ç™»å…¥å¤±æ•—ï¼š{e}")
        return False


# --- ç™»å…¥ Threads (Instagram)
# def login_to_threads(driver):
#     IG_USERNAME = "leafwann_"
#     IG_PASSWORD = "threads2025"

#     driver.get("https://www.threads.net/login")
#     try:
#         login_button = WebDriverWait(driver, 15).until(
#             EC.presence_of_element_located((By.XPATH, "//span[text()='ä½¿ç”¨ Instagram å¸³è™Ÿç¹¼çºŒ']"))
#         )
#         login_button.click()
#     except:
#         print("âŒ æ‰¾ä¸åˆ°ç™»å…¥æŒ‰éˆ•")
#         return False

#     try:
#         WebDriverWait(driver, 15).until(EC.presence_of_element_located((By.NAME, "username")))
#         driver.find_element(By.NAME, "username").send_keys(IG_USERNAME)
#         driver.find_element(By.NAME, "password").send_keys(IG_PASSWORD)
#         driver.find_element(By.NAME, "password").send_keys(Keys.ENTER)
#         WebDriverWait(driver, 20).until(
#             EC.presence_of_element_located((By.XPATH, "//div[@data-pressable-container='true']"))
#         )
#         return True
#     except Exception as e:
#         print("âŒ ç™»å…¥å¤±æ•—ï¼š", e)
#         return False

# --- æ“·å–ç•™è¨€ï¼ˆé¿é–‹ä¸»æ–‡ã€åªæŠ“æŒ‡å®šç•™è¨€å®¹å™¨ï¼‰
def scrape_comments_from_post_page(driver):
    comments = []
    try:
        wrapper = driver.find_element(By.XPATH, "//div[contains(@class,'xb57i2i') and contains(@class,'x1q594ok')]")
        comment_blocks = wrapper.find_elements(By.XPATH, ".//div[contains(@class, 'x1a6qonq')]")[1:]
        for block in comment_blocks:
            spans = block.find_elements(By.XPATH, ".//span[@dir='auto']/span")
            text = "\n".join([s.text.strip() for s in spans if s.text.strip()])
            if text:
                comments.append(text)
            if len(comments) >= 10:
                break
    except Exception as e:
        print("ç•™è¨€æ“·å–éŒ¯èª¤ï¼š", e)
    return comments


# ç”¢ç”Ÿä¸»é¡Œ titleï¼ˆä½¿ç”¨ jiebaï¼‰
def generate_title_with_keywords(text, topk=3):
    """
    å¾ä¸€æ®µæ–‡å­—ä¸­æŠ½å–é—œéµè©ä½œç‚ºä¸»é¡Œã€‚
    Args:
        text (str): è²¼æ–‡å…§å®¹
        topk (int): é¸å¹¾å€‹é—œéµè©çµ„æˆä¸»é¡Œ
    Returns:
        str: ç”±é—œéµè©çµ„æˆçš„ä¸»é¡Œï¼Œä¾‹å¦‚ã€Œæ²–ç¹©ï½œæ¨è–¦ï½œæµ·ã€
    """
    keywords = jieba.analyse.extract_tags(text, topK=topk)
    return "ï½œ".join(keywords) if keywords else "ç„¡æ³•ç”¢ç”Ÿä¸»é¡Œ"

def count_chinese_chars(text):
    return len(re.findall(r'[\u4e00-\u9fff]', text))

def is_mostly_chinese(text, threshold=0.6):
    chinese_chars = count_chinese_chars(text)
    return (chinese_chars / max(len(text), 1)) >= threshold

def scrape_threads_by_keyword(keyword):
    keyword_to_search = keyword

    MAX_TARGET = 15
    MAX_SCROLLS = 100
    MAX_NO_NEW_SCROLLS = 10
    MAX_DAYS = 7

    time_cutoff = datetime.now(tz=tz.gettz("Asia/Taipei")) - timedelta(days=MAX_DAYS)

    start_time = datetime.now()
    options = webdriver.ChromeOptions()
    options.add_argument('--headless')
    options.add_argument("--disable-gpu")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("user-agent=Mozilla/5.0")
    driver = webdriver.Chrome(options=options)

    try:
        if not login_to_threads(driver):
            return

        driver.get("https://www.threads.com/search?hl=zh-tw")
        search_input = WebDriverWait(driver, 15).until(
            EC.presence_of_element_located((By.XPATH, "//input[@type='search' and @placeholder='æœå°‹']"))
        )
        search_input.send_keys(keyword_to_search)
        search_input.send_keys(Keys.ENTER)
        WebDriverWait(driver, 15).until(
            EC.presence_of_element_located((By.XPATH, "//div[@data-pressable-container='true']"))
        )

        # âœ… é»é¸ã€Œæœ€è¿‘ã€Tab
        try:
            recent_tab = WebDriverWait(driver, 10).until(
                EC.element_to_be_clickable((By.XPATH, "//a[@aria-label='æœ€è¿‘']"))
            )
            recent_tab.click()
            print("ğŸ•“ å·²é»é¸ã€æœ€è¿‘ã€Tab")
            time.sleep(2)
        except Exception as e:
            print(f"âš ï¸ é»é¸ã€æœ€è¿‘ã€Tab å¤±æ•—ï¼š{e}")

        all_posts_data = []
        post_index = 0
        scroll_round = 0
        no_new_scrolls = 0
        last_post_count = 0
        main_window_handle = driver.current_window_handle

        while len(all_posts_data) < MAX_TARGET and scroll_round < MAX_SCROLLS:
            scroll_round += 1
            driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(random.uniform(2, 4))

            posts = driver.find_elements(By.XPATH, "//div[@data-pressable-container='true']")
            new_posts_found = len(posts) > last_post_count
            last_post_count = len(posts)

            post_index = 0  # æ¯æ¬¡æ»¾å‹•å¾Œå¾æ–°ä¸€è¼ªè²¼æ–‡é–‹å§‹

            while post_index < len(posts) and len(all_posts_data) < MAX_TARGET:
                try:
                    post = posts[post_index]
                except IndexError:
                    break  # è¶…å‡ºè²¼æ–‡åˆ—è¡¨é•·åº¦
                post_index += 1

                try:
                    print(f"\nğŸ“ æ­£åœ¨è™•ç†ç¬¬ {post_index} ç¯‡è²¼æ–‡")

                    # ç™¼æ–‡æ™‚é–“éæ¿¾
                    try:
                        raw_time = post.find_element(By.XPATH, ".//time").get_attribute("datetime")
                        taipei_time = parser.parse(raw_time).astimezone(tz.gettz("Asia/Taipei"))
                        if taipei_time < time_cutoff:
                            print(f"ğŸ“… ç™¼æ–‡æ™‚é–“ {taipei_time.strftime('%Y-%m-%d %H:%M:%S')} è¶…å‡ºæ—¥æœŸå€é–“ï¼Œè·³é")
                            continue
                        post_time = taipei_time.strftime("%Y-%m-%d")
                    except Exception as e:
                        print(f"â³ ç„¡æ³•è§£ææ™‚é–“ï¼Œè·³éï¼ˆéŒ¯èª¤ï¼š{e}ï¼‰")
                        continue

                    # å…§æ–‡æŠ“å–
                    try:
                        spans = post.find_elements(By.XPATH, ".//div[contains(@class,'x1a6qonq') and contains(@class,'x6ikm8r')]//span[contains(@class,'x1lliihq')]//span")
                        parts = [s.text.strip() for s in spans if s.text.strip()]
                        post_text = "\n".join(sorted(set(parts), key=parts.index))
                    except:
                        print("âŒ æŠ“å–è²¼æ–‡å…§å®¹å¤±æ•—ï¼Œè·³é")
                        continue

                    if not is_mostly_chinese(post_text):
                        print("ğŸŒ éä¸­æ–‡ä¸»é«”è²¼æ–‡ï¼Œè·³é")
                        continue
                    if count_chinese_chars(post_text) < 30:
                        print("ğŸ”¡ ä¸­æ–‡å­—æ•¸ä¸è¶³ 30ï¼Œè·³é")
                        continue


                    try:
                        permalink = post.find_element(By.XPATH, ".//a[@role='link'][time]").get_attribute("href")
                        post_link = "https://www.threads.com" + permalink if permalink.startswith("/") else permalink
                    except:
                        post_link = "N/A"

                    post_title = generate_title_with_keywords(post_text)

                    comments_data = []
                    if post_link != "N/A":
                        try:
                            driver.execute_script("window.open(arguments[0]);", post_link)
                            time.sleep(2)
                            new_tab = [w for w in driver.window_handles if w != main_window_handle][-1]
                            driver.switch_to.window(new_tab)
                            WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.TAG_NAME, "body")))
                            comments_data = scrape_comments_from_post_page(driver)
                            driver.close()
                            driver.switch_to.window(main_window_handle)
                        except Exception as e:
                            print("ç•™è¨€æŠ“å–éŒ¯èª¤ï¼š", e)
                            if len(driver.window_handles) > 1:
                                driver.close()
                            driver.switch_to.window(main_window_handle)

                    all_posts_data.append({
                        'title': post_title,
                        'date': post_time,
                        'post_url': post_link,
                        'summary': post_text,
                        'comments': comments_data,
                        'source': 'Threads',
                    })

                    print(f"âœ… æ”¶éŒ„ç¬¬ {len(all_posts_data)} ç¯‡è²¼æ–‡")
                    time.sleep(random.uniform(1, 2))

                except StaleElementReferenceException:
                    print(f"âš ï¸ ç¬¬ {post_index} ç¯‡è²¼æ–‡å¤±æ•ˆï¼ˆStaleElementï¼‰ï¼Œè·³é")
                    continue

            if not new_posts_found:
                no_new_scrolls += 1
                if no_new_scrolls >= MAX_NO_NEW_SCROLLS:
                    print("âš ï¸ å¤šæ¬¡æ»¾å‹•ç„¡æ–°å…§å®¹ï¼Œåœæ­¢ã€‚")
                    break
            else:
                no_new_scrolls = 0


        if len(all_posts_data) < MAX_TARGET:
            print(f"âš ï¸ è²¼æ–‡æ•¸æœªé” {MAX_TARGET}ï¼Œåƒ…æ”¶éŒ„ {len(all_posts_data)} ç¯‡")

        elapsed = datetime.now() - start_time

        print(f"ğŸ“Š å…±æŠ“å– {len(all_posts_data)} ç­†ï¼Œè€—æ™‚ {elapsed.seconds // 60} åˆ† {elapsed.seconds % 60} ç§’")
        return all_posts_data

    finally:
        print("ğŸ§¹ é—œé–‰ç€è¦½å™¨...")
        driver.quit()
        print("âœ… çµæŸ")


# # --- åŸ·è¡Œ

if __name__ == "__main__":
    keyword = ''
    posts = scrape_threads_by_keyword(keyword)
    if posts:
        print(posts[0])