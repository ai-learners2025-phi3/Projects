import requests
from bs4 import BeautifulSoup
import matplotlib.pyplot as plt
import os
from datetime import datetime, timedelta
import re
import jieba
import jieba.analyse
from collections import Counter, defaultdict

from snownlp import SnowNLP
from wordcloud import WordCloud
import google.generativeai as genai


BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))

def parse_date(date_str):
    date_str = date_str.strip()
    now = datetime.now()

    # å¾æ··äº‚å­—ä¸²ä¸­æŠ½å– YYYY-MM-DD æ ¼å¼
    match = re.search(r'\d{4}-\d{2}-\d{2}', date_str)
    if match:
        return match.group()

    # æ”¯æ´å¤šç¨®æ ¼å¼è§£æ
    date_formats = [
        "%Y/%m/%d",
        "%Y/%m/%d %H:%M",
        "%Y-%m-%d",
        "%Yå¹´%mæœˆ%dæ—¥",
    ]
    for fmt in date_formats:
        try:
            date = datetime.strptime(date_str, fmt)
            return date.strftime("%Y-%m-%d")
        except Exception:
            continue

    # ç›¸å°æ™‚é–“è™•ç†
    if "å‰›å‰›" in date_str or "ç§’å‰" in date_str:
        return now.strftime("%Y-%m-%d")
    
    minute_match = re.search(r"(\d+)\s*åˆ†é˜å‰", date_str)
    if minute_match:
        date = now - timedelta(minutes=int(minute_match.group(1)))
        return date.strftime("%Y-%m-%d")
    
    hour_match = re.search(r"(\d+)\s*å°æ™‚å‰", date_str)
    if hour_match:
        date = now - timedelta(hours=int(hour_match.group(1)))
        return date.strftime("%Y-%m-%d")

    day_match = re.search(r"(\d+)\s*å¤©å‰", date_str)
    if day_match:
        date = now - timedelta(days=int(day_match.group(1)))
        return date.strftime("%Y-%m-%d")

    # ç„¡æ³•è§£æå°±åŸæ¨£å›å‚³
    return date_str

def extract_tags(text, top_k=10, use_tfidf=True):
    """
    å¾ä¸€æ®µä¸­æ–‡æ–‡å­—ä¸­æ“·å–é—œéµå­—è©
    :param text: è¼¸å…¥çš„åŸå§‹æ–‡å­—
    :param top_k: æœ€å¤šæ“·å–å¹¾å€‹é—œéµå­—ï¼ˆåªæœ‰åœ¨ use_tfidf=True æ™‚ç”Ÿæ•ˆï¼‰
    :param use_tfidf: æ˜¯å¦ä½¿ç”¨ TF-IDF æ¬Šé‡é¸å­—ï¼ˆå¦å‰‡å°±æ˜¯ç´”åˆ†è©ï¼‰
    :param stopwords: åœç”¨è©æ¸…å–®ï¼ˆå¯ä»¥å®¢è£½ï¼‰
    :return: å­—è©æ¨™ç±¤çš„ list
    """
    stopwords = set([
      'çš„', 'äº†', 'æ˜¯', 'æˆ‘', 'ä½ ', 'ä»–', 'å¥¹', 'å®ƒ', 'æˆ‘å€‘', 'ä½ å€‘', 'ä»–å€‘', 'é€™', 'é‚£', 'å’Œ', 'èˆ‡',
      'åœ¨', 'ä¸', 'æœ‰', 'ä¹Ÿ', 'å°±', 'éƒ½', 'å¾ˆ', 'è€Œ', 'åŠ', 'æˆ–', 'è¢«', 'é‚„', 'èƒ½', 'æœƒ',
    ])
    if use_tfidf:
        # ä½¿ç”¨ TF-IDF æŠ½å–é—œéµè©
        tags = jieba.analyse.extract_tags(text, topK=top_k)
    else:
        # åŸºæœ¬æ–·è©
        tags = jieba.lcut(text)

    # éæ¿¾æ¨™é»ã€ç©ºå­—å…ƒã€åœç”¨è©
    filtered_tags = []
    for word in tags:
        word = word.strip()
        if word and re.match(r'^[\u4e00-\u9fff]+$', word) and word not in stopwords:
            filtered_tags.append(word)

    return filtered_tags

def get_tvbs_news(keyword, max_pages=4):
    results = []

    for page in range(1, max_pages + 1):
        url = f"https://news.tvbs.com.tw/news/searchresult/{keyword}/news/{page}"
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"
        }
        # ç™¼é€ GET è«‹æ±‚
        response = requests.get(url, headers=headers)
        if response.status_code != 200:
            continue
        html = BeautifulSoup(response.text, "html.parser")

        # æŠ“å–æ–°èå€å¡Š
        article_list = html.find('main').find('div', class_='list').find_all('li')
        for article in article_list:
            a_tag = article.find('a')
            if not a_tag:
                continue    
            # æ¨™é¡Œ
            title_tag = article.find('h2', class_='txt')
            title = title_tag.text.strip() if title_tag else ''

            # æ–°èé€£çµ
            news_url = a_tag['href'] if a_tag.has_attr('href') else ''

            # ç™¼å¸ƒæ™‚é–“
            time_tag = article.find('div', class_='time')
            date = time_tag.text.strip() if time_tag else ''

            # æ‘˜è¦
            summary_tag = article.find('div', class_='summary')
            summary = summary_tag.text.strip() if summary_tag else ''

            # æ¨™ç±¤
            tags_raw = a_tag.get('data-news_tag', '[]')
            # è™•ç†æˆä¹¾æ·¨çš„ listï¼ˆç§»é™¤ ' èˆ‡ç©ºæ ¼ï¼‰
            tags = [tag.strip(" '") for tag in tags_raw.strip('[]').split(',')]

            # é¡åˆ¥
            category_tag = article.find('div', class_='type').find('a')
            category = category_tag.text.strip() if category_tag else ''

            # åŠ å…¥çµæœ
            results.append({
                'title': title,
                'date': parse_date(date),
                'summary': summary,
                'news_tag': tags,
                'news_url': news_url,
                'category': category,
                'source':'TVBSæ–°èç¶²',
            })
    return results

def get_chdtv_news(keyword, max_pages=3):
    results = []
    for page in range(1, max_pages + 1):
        url = f"https://www.chinatimes.com/search/{keyword}?page={page}&chdtv"
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"
        }
        # ç™¼é€ GET è«‹æ±‚
        response = requests.get(url, headers=headers)
        if response.status_code != 200:
            continue
        html = BeautifulSoup(response.text, "html.parser")

        # æŠ“å–æ–°èå€å¡Š
        article_list = html.find('div', class_='wrapper').find('ul', class_='vertical-list list-style-none').find_all('li')
        for article in article_list:
            a_tag = article.find('a')
            if not a_tag:
                continue    
            # æ¨™é¡Œ
            title_tag = article.find('h3', class_='title')
            title = title_tag.text.strip() if title_tag else ''

            # æ–°èé€£çµ
            news_url = a_tag['href'] if a_tag.has_attr('href') else ''

            # ç™¼å¸ƒæ™‚é–“
            time_tag = article.find('span', class_='date')
            date = time_tag.text.strip() if time_tag else ''

            # æ‘˜è¦
            summary_tag = article.find('p', class_='intro')
            summary = summary_tag.text.strip() if summary_tag else ''

            # æ¨™ç±¤
            tags = extract_tags(summary)

            # åŠ å…¥çµæœ
            results.append({
                'title': title,
                'date': parse_date(date),
                'summary': summary,
                'news_tag': tags,
                'news_url': news_url,
                'source':'ä¸­æ™‚æ–°èç¶²',
            })
    return results

def get_now_news(keyword, max_pages=5):
    results = []
    for page in range(1, max_pages + 1):
        url = f"https://www.nownews.com/search?q={keyword}&page={page}"
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"
        }
        # ç™¼é€ GET è«‹æ±‚
        response = requests.get(url, headers=headers)
        if response.status_code != 200:
            continue
        html = BeautifulSoup(response.text, "html.parser")

        # æŠ“å–æ–°èå€å¡Š
        article_list = html.find('div', class_='mainBlk').find('div', class_='item-list').find_all('a')
        for article in article_list:
            a_tag = article.find('a')
            if not a_tag:
                continue    
            # æ¨™é¡Œ
            title_tag = article.find('h3', class_='title')
            title = title_tag.text.strip() if title_tag else ''

            # æ–°èé€£çµ
            news_url = a_tag['href'] if a_tag.has_attr('href') else ''

            # ç™¼å¸ƒæ™‚é–“
            time_tag = article.find('p', class_='time').find('time')
            date = time_tag.text.strip() if time_tag else ''

            # æ‘˜è¦
            summary_tag = article.find('p', class_='content text-truncate')
            summary = summary_tag.text.strip() if summary_tag else ''

            # æ¨™ç±¤
            tags = extract_tags(summary)

            # åŠ å…¥çµæœ
            results.append({
                'title': title,
                'date': parse_date(date),
                'summary': summary,
                'news_tag': tags,
                'news_url': news_url,
                'source':'NOWnews',
            })
    return results

def get_ET_news(keyword, max_pages=10):
    results = []

    for page in range(1, max_pages + 1):
        url = f"https://www.ettoday.net/news_search/doSearch.php?keywords={keyword}&idx=1&page={page}"
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"
        }

        response = requests.get(url, headers=headers)
        if response.status_code != 200:
            continue

        html = BeautifulSoup(response.text, "html.parser")

        # æ ¹æ“šå¯¦éš›ç¶²é çµæ§‹å®šä½æ–‡ç« å€å¡Š
        article_list = html.select("div.archive.clearfix")

        for article in article_list:
            a_tag = article.find("a")
            if not a_tag:
                continue

            # æ¨™é¡Œ
            title_tag = article.find("h2")
            title = title_tag.text.strip() if title_tag else ""

            # æ–°èé€£çµ
            news_url = a_tag["href"] if a_tag.has_attr("href") else ""

            # ç™¼å¸ƒæ™‚é–“
            time_tag = article.select_one('.date')
            date = time_tag.text.strip() if time_tag else ''

            # æ‘˜è¦
            summary_tag = article.find("p")
            summary = summary_tag.text.strip() if summary_tag else ""

            # æ¨™ç±¤
            tags = extract_tags(summary)

            # é¡åˆ¥
            category_tag = article.find('span', class_='date').find('a')
            category = category_tag.text.strip() if category_tag else ''

            results.append({
                "title": title,
                "date": parse_date(date),
                "summary": summary,
                "news_tag": tags,
                "news_url": news_url,
                "category": category,
                "source": "ETtodayæ–°èé›²",
            })

    return results

def search_news(keyword):
    tvbs_news = get_tvbs_news(keyword)
    ET_news = get_ET_news(keyword)
    # chdtv_news = get_chdtv_news(keyword)
    articles = tvbs_news + ET_news
    return articles

def analyze_sentiment(articles):
    """
    ä½¿ç”¨ SnowNLP åˆ†ææƒ…ç·’
    :param articles: æ¯ç¯‡æ–‡ç« ç‚º dictï¼Œéœ€åŒ…å« summary å’Œ title
    :return: å›å‚³åŸå§‹é™£åˆ—ï¼Œæ¯ç¯‡æ–‡ç« åŠ å…¥ sentiment_score åŠ sentiment æ¨™ç±¤ï¼ˆæ­£é¢ / è² é¢ / ä¸­ç«‹ï¼‰
    """
    for article in articles:
        # å…ˆä½¿ç”¨ summaryï¼Œè‹¥ç‚ºç©ºå‰‡ç”¨ title
        text = article['summary'] if article.get('summary') else article.get('title', '')
        
        s = SnowNLP(text)
        score = s.sentiments  # åˆ†æ•¸ä»‹æ–¼ 0~1ï¼Œæ„ˆæ¥è¿‘ 1 è¶Šæ­£é¢ï¼Œæ¥è¿‘ 0 è¶Šè² é¢
        article['sentiment_score'] = score

        # åŠ å…¥ä¸­ç«‹åˆ¤æ–·é‚è¼¯ï¼š0~0.4 è² é¢ã€0.4~0.6 ä¸­ç«‹ã€0.6~1 æ­£é¢
        if score >= 0.6:
            article['sentiment'] = 'æ­£é¢'   # æ­£é¢
        elif score <= 0.4:
            article['sentiment'] = 'è² é¢'  # è² é¢
        else:
            article['sentiment'] = 'ä¸­ç«‹'   # ä¸­ç«‹

    return articles

def count_sentiment(articles):
    sentiment_count = {
        'æ­£é¢': sum(1 for a in articles if a['sentiment'] == 'æ­£é¢'),
        'è² é¢': sum(1 for a in articles if a['sentiment'] == 'è² é¢'),
        'ä¸­ç«‹': sum(1 for a in articles if a['sentiment'] == 'ä¸­ç«‹'),
    }
    return sentiment_count

def get_top_words(articles, top_n=5):
    """
    æ ¹æ“šæ–‡ç« æƒ…ç·’æ¨™ç±¤ï¼Œçµ±è¨ˆæ­£é¢ã€è² é¢ã€ä¸­ç«‹æ‘˜è¦ä¸­çš„é«˜é »è©ã€‚
    
    :param articles: åŒ…å« 'summary' èˆ‡ 'sentiment' æ¬„ä½çš„æ–‡ç« åˆ—è¡¨
    :param top_n: æ¯å€‹æƒ…ç·’é¡åˆ¥ä¸­é¡¯ç¤ºçš„å‰ N åé«˜é »è©
    :return: dictï¼ŒåŒ…å«ä¸‰é¡è©å½™çš„ top_n çµæœ
    """
    pos_words, neg_words, neu_words = [], [], []

    for article in articles:
        words = extract_tags(article['summary'])
        if article['sentiment'] == 'æ­£é¢':
            pos_words.extend(words)
        elif article['sentiment'] == 'è² é¢':
            neg_words.extend(words)
        elif article['sentiment'] == 'ä¸­ç«‹':
            neu_words.extend(words)

    return {
        'positive': Counter(pos_words).most_common(top_n),
        'negative': Counter(neg_words).most_common(top_n),
        'neutral': Counter(neu_words).most_common(top_n)
    }

def sentiment_by_category(articles):
    stats = defaultdict(lambda: {'positive': 0, 'neutral': 0, 'negative': 0})
    for art in articles:
        cate = art['category']
        if art['sentiment'] == 'æ­£é¢':
            stats[cate]['positive'] += 1
        elif art['sentiment'] == 'ä¸­ç«‹':
            stats[cate]['neutral'] += 1
        elif art['sentiment'] == 'è² é¢':
            stats[cate]['negative'] += 1
    return dict(stats)

def generate_wordcloud(tags, save_path):
    font_path = "/System/Library/Fonts/STHeiti Medium.ttc"  # macOS å¯ç”¨å­—é«”
    text = ' '.join(tags)
    wc = WordCloud(
        background_color="white", 
        font_path=font_path, 
        width=1096, 
        height=480,
        max_words=100,
    )
    wc.generate(text)
    wc.to_file(save_path)

def news_counter(articles):
    # å°‡è³‡æ–™æ•´ç†æˆæ¯æ—¥æ•¸é‡ dict
    daily_counts = Counter()
    for article in articles:
        if article['date']:
            daily_counts[article['date']] += 1

    # è½‰ç‚ºæ’åºå¾Œçš„ x, y list
    trend_labels = sorted(daily_counts.keys())
    trend_values = [daily_counts[date] for date in trend_labels]
    return trend_labels,trend_values

def generate_prompt(keyword, sentiment_count, top_words, sentiment_by_cat):
    prompt = f"""
    è«‹æ ¹æ“šä»¥ä¸‹è³‡æ–™ï¼Œæ’°å¯«ä¸€ç¯‡åˆ†æå ±å‘Šï¼Œå­—æ•¸ç´„ 300 å­—ï¼Œä»¥æ•¸æ“šåˆ†æå¸«çš„è§’åº¦å»åˆ†æï¼Œèªæ°£å°ˆæ¥­æ¸…æ¥šï¼š
    
    ğŸ” ä¸»é¡Œï¼š{keyword}

    ğŸ“Š æƒ…ç·’æ¯”ä¾‹ï¼š
    æ­£é¢æ–‡ç« æ•¸ï¼š{sentiment_count.get('æ­£é¢', 0)}
    è² é¢æ–‡ç« æ•¸ï¼š{sentiment_count.get('è² é¢', 0)}
    ä¸­ç«‹æ–‡ç« æ•¸ï¼š{sentiment_count.get('ä¸­ç«‹', 0)}

    ğŸ“Œ é¡åˆ¥æƒ…ç·’çµ±è¨ˆæ¦‚æ³ï¼š
    {sentiment_by_cat}

    ğŸ”¥ é«˜é »é—œéµè©(æ¯å€‹è©åœ¨æ–‡ç« å‡ºç¾çš„æ•¸é‡):
    {"ã€".join(top_words.get('all', []))}

    è«‹ç¶œåˆä»¥ä¸Šè³‡è¨Šï¼Œèªªæ˜ç›®å‰ç†±åº¦è¶¨å‹¢èˆ‡ç¤¾ç¾¤é—œæ³¨é‡é»ã€‚
    """
    return prompt

def call_LLM(prompt):
    api_key = ''
    genai.configure(api_key = api_key)
    model = genai.GenerativeModel('gemini-1.5-flash') # gemini-1.5-flash/ gemini-1.5-flash
    response = model.generate_content(prompt)
    return response.text.strip()

def work(keyword):
    start_time = datetime.now().strftime("%Y%m%d_%H%M")
    # 1. æœå°‹èˆ‡æƒ…ç·’åˆ†æ
    articles = analyze_sentiment(search_news(keyword))
    # 2. è¨ˆç®—æ­£è² æƒ…ç·’æ•¸é‡
    sentiment_count = count_sentiment(articles)
    # 3. è¶¨å‹¢åˆ†æï¼ˆå„æ™‚é–“é»çš„æ–°èæ•¸é‡ï¼‰
    trend_labels,trend_values = news_counter(articles)
    # 4. åˆ†æè©å½™è²¢ç»
    top_word = get_top_words(articles)
    # 5. åˆ†æåˆ†é¡æƒ…ç·’
    category_stats = sentiment_by_category(articles)
    # 6. çµ±è¨ˆæ¨™ç±¤è©å½™è£½ä½œæ–‡å­—é›²åœ–
    all_tags = []
    for art in articles:
        all_tags.extend(art['news_tag'])
    
    wordcloud_path = os.path.join(BASE_DIR, 'static', 'clouds', f'{keyword}{start_time}.png')
    os.makedirs(os.path.dirname(wordcloud_path), exist_ok=True)
    generate_wordcloud(all_tags, wordcloud_path)
    # 7. ä½¿ç”¨ Gemini ç”Ÿæˆå ±å‘Š
    prompt = generate_prompt(keyword, sentiment_count, top_word, category_stats)
    try:
        report = call_LLM(prompt)
    except Exception as e:
        report = f"âš ï¸ Gemini å›æ‡‰å¤±æ•—ï¼š{e}"

    end_time = datetime.now().strftime("%Y%m%d_%H%M")
    # å›å‚³çµæœ
    return {
        'articles': articles,
        'sentiment_count': sentiment_count,
        'top_word': top_word,
        'category_stats': category_stats,
        'tag_image': f'./static/clouds/{keyword}{start_time}.png',
        'trend_labels': trend_labels,
        'trend_values': trend_values,
        'AIreport': report,
    }

# source venv/bin/activate
# python manage.py runserver