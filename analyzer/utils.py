# ğŸ“¦ ç³»çµ±èˆ‡ç’°å¢ƒè®Šæ•¸
import os
from django.conf import settings

# ğŸ•’ æ™‚é–“èˆ‡æ—¥æœŸè™•ç†
from datetime import datetime, timedelta

# ğŸŒ ç¶²è·¯è«‹æ±‚èˆ‡è³‡æ–™çˆ¬å–
import requests
from bs4 import BeautifulSoup

# ğŸ“Š è³‡æ–™è™•ç†èˆ‡åˆ†æ
import pandas as pd
from collections import Counter, defaultdict
import pymysql
import json 

# ğŸ§  è‡ªç„¶èªè¨€è™•ç†ï¼ˆNLPï¼‰
import jieba
import jieba.analyse
from snownlp import SnowNLP

# ğŸ–¼ï¸ è¦–è¦ºåŒ–èˆ‡åœ–å½¢ç”¢ç”Ÿ
import matplotlib.pyplot as plt
from wordcloud import WordCloud

# ğŸ¤– Google Gemini AI æœå‹™
import google.generativeai as genai

# ğŸ§ª æ–‡å­—è™•ç†èˆ‡æ­£è¦è¡¨ç¤ºå¼
import re

from .models import News, Posts, AnalysisResult
from .ptt_crawler import get_ptt_posts,ptt_keyword
from .threads_crawler import scrape_threads_by_keyword



BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))

def _save_single_news_article(news_data_item, history_search_instance=None):
    """
    ä¿å­˜å–®ç¯‡æ–°èæ–‡ç« åˆ° News è¡¨ã€‚
    news_data_item: å­—å…¸ï¼ŒåŒ…å«å–®ç¯‡æ–°èè³‡è¨Šã€‚
    history_search_instance: HistorySearch ç‰©ä»¶ï¼Œå¯é¸ï¼Œç”¨æ–¼ ManyToMany é—œè¯ã€‚
    """
    try:
        date = datetime.strptime(news_data_item.get('date'), "%Y-%m-%d")
    except:
        date = datetime.now
    try:
        news_article, created = News.objects.update_or_create(
            url=news_data_item['news_url'], # ä½¿ç”¨ URL ä½œç‚ºå”¯ä¸€æ¨™è­˜
            defaults={
                'keyword': news_data_item.get('keyword', 'æ–°è'),
                'source': news_data_item.get('source', ''),
                'title': news_data_item.get('title', ''),
                'publish_date': date,
                'summary': news_data_item.get('summary', ''),
                'tags': news_data_item.get('news_tag', ''),
                'category': news_data_item.get('category', ''),
                'sentiment': news_data_item.get('sentiment', ''),
                'sentiment_score': news_data_item.get('sentiment_score'),
            }
        )
        if history_search_instance:
            news_article.searches.add(history_search_instance)
        print(f"âœ… News - {'å‰µå»º' if created else 'æ›´æ–°'}ï¼š{news_article.title}")
    except Exception as e:
        print(f"âŒ News å„²å­˜å¤±æ•— ({news_data_item.get('title', 'æœªçŸ¥')}): {e}")

def _save_single_post_item(post_data_item, history_search_instance=None):
    """
    ä¿å­˜å–®ç¯‡è²¼æ–‡åˆ° Posts è¡¨ã€‚
    post_data_item: å­—å…¸ï¼ŒåŒ…å«å–®ç¯‡è²¼æ–‡è³‡è¨Šã€‚
    history_search_instance: HistorySearch ç‰©ä»¶ï¼Œå¯é¸ï¼Œç”¨æ–¼ ManyToMany é—œè¯ã€‚
    """
    try:
        date = datetime.strptime(post_data_item.get('date'), "%Y-%m-%d")
    except:
        date = datetime.now
    try:
        post_item, created = Posts.objects.update_or_create(
            url=post_data_item['post_url'], # ä½¿ç”¨ URL ä½œç‚ºå”¯ä¸€æ¨™è­˜
            defaults={
                'keyword': post_data_item.get('keyword', 'None'),
                'source': post_data_item.get('source', ''),
                'title': post_data_item.get('title', ''),
                'publish_date': date,
                'summary': post_data_item.get('summary', ''),
                'comments': post_data_item.get('comments', []),
                'sentiment': post_data_item.get('sentiment', ''),
                'sentiment_score': post_data_item.get('sentiment_score'),
            }
        )
        if history_search_instance:
            post_item.searches.add(history_search_instance)
        print(f"âœ… Post - {'å‰µå»º' if created else 'æ›´æ–°'}ï¼š{post_item.title}")
    except Exception as e:
        print(f"âŒ Post å„²å­˜å¤±æ•— ({post_data_item.get('title', 'æœªçŸ¥')}): {e}")

def _save_analysis_result(analysis_n,analysis_p, history_search_instance,current_keyword):
    """
    ä¿å­˜åˆ†æçµæœåˆ° AnalysisResult è¡¨ã€‚
    analysis_data: å­—å…¸ï¼ŒåŒ…å«æ‰€æœ‰åˆ†æçµæœã€‚
    history_search_instance: ç›¸é—œçš„ HistorySearch ç‰©ä»¶ (å¯èƒ½ç‚º None)ã€‚
    current_keyword: ç•¶å‰åˆ†æçš„é—œéµå­—ã€‚
    """
    try:
        # identifier_display å°‡å§‹çµ‚ä½¿ç”¨ history_search_instance.keyword
        identifier_display = history_search_instance.keyword 

        # ç”±æ–¼ history_search_instance ç¸½æœƒå­˜åœ¨ï¼Œæˆ‘å€‘ç›´æ¥ç”¨å®ƒä¾†æŸ¥æ‰¾/æ›´æ–° AnalysisResult
        analysis_result, created = AnalysisResult.objects.update_or_create(
            search=history_search_instance, # ä½¿ç”¨æœ‰æ•ˆçš„ history_search_instance
            defaults={
                'keyword': current_keyword,
                'positive_count': analysis_n.get('positive_count', 0),
                'negative_count': analysis_n.get('negative_count', 0),
                'neutral_count': analysis_n.get('neutral_count', 0),
                'cate_count': analysis_n.get('cate_count', {}),
                'tag_image': analysis_n.get('tag_image', ''),
                'top_word': analysis_n.get('top_word', {}),
                'trend_labels': analysis_n.get('trend_labels', []),
                'trend_values': analysis_n.get('trend_values', []),
                'report': analysis_n.get('report', ''),

                'pos_count': analysis_p.get('pos_count', 0),
                'neg_count': analysis_p.get('neg_count', 0),
                'neu_count': analysis_p.get('neu_count', 0),
                'sour_count': analysis_p.get('sour_count', {}),
                'post_image': analysis_p.get('post_image', ''),
                'post_top_word': analysis_p.get('post_top_word', {}),
                'post_trend_labels': analysis_p.get('post_trend_labels', []),
                'post_trend_values': analysis_p.get('post_trend_values', []),
                'post_report': analysis_p.get('post_report', ''),
            }
        )
        print(f"âœ… AnalysisResult - {'å‰µå»º' if created else 'æ›´æ–°'}ï¼š{identifier_display} é—œéµå­—")
        return analysis_result
    except Exception as e:
        # éŒ¯èª¤æ—¥èªŒä¹Ÿç›´æ¥ä½¿ç”¨ history_search_instance.keyword
        print(f"âŒ AnalysisResult å„²å­˜å¤±æ•— ({history_search_instance.keyword}): {e}")
        return None

def _batch_save_news(news_list_data, history_search_instance=None):
    """
    æ‰¹é‡ä¿å­˜æ–°èåˆ—è¡¨æ•¸æ“šã€‚
    news_list_data: åŒ…å«å¤šå€‹æ–°èå­—å…¸çš„åˆ—è¡¨ã€‚
    history_search_instance: HistorySearch ç‰©ä»¶ï¼Œå¯é¸ã€‚
    """
    for item_data in news_list_data:
        _save_single_news_article(item_data, history_search_instance)

def _batch_save_posts(posts_list_data, history_search_instance=None):
    """
    æ‰¹é‡ä¿å­˜è²¼æ–‡åˆ—è¡¨æ•¸æ“šã€‚
    posts_list_data: åŒ…å«å¤šå€‹è²¼æ–‡å­—å…¸çš„åˆ—è¡¨ã€‚
    history_search_instance: HistorySearch ç‰©ä»¶ï¼Œå¯é¸ã€‚
    """
    for item_data in posts_list_data:
        _save_single_post_item(item_data, history_search_instance)

def parse_date(date_str, as_datetime=False):
    date_str = date_str.strip()
    now = datetime.now()

    # å˜—è©¦å¾å­—ä¸²ä¸­æŠ½å‡º YYYY-MM-DD æ ¼å¼
    match = re.search(r'\d{4}-\d{2}-\d{2}', date_str)
    if match:
        date = datetime.strptime(match.group(), "%Y-%m-%d")
        return date if as_datetime else date.strftime("%Y-%m-%d")

    # æ”¯æ´å¤šç¨®å¸¸è¦‹æ—¥æœŸæ ¼å¼
    date_formats = [
        "%Y/%m/%d",
        "%Y/%m/%d %H:%M",
        "%Y-%m-%d",
        "%Yå¹´%mæœˆ%dæ—¥",
    ]
    for fmt in date_formats:
        try:
            date = datetime.strptime(date_str, fmt)
            return date if as_datetime else date.strftime("%Y-%m-%d")
        except Exception:
            continue

    # ç›¸å°æ™‚é–“è™•ç†
    if "å‰›å‰›" in date_str or "ç§’å‰" in date_str:
        return now if as_datetime else now.strftime("%Y-%m-%d")

    minute_match = re.search(r"(\d+)\s*åˆ†é˜å‰", date_str)
    if minute_match:
        date = now - timedelta(minutes=int(minute_match.group(1)))
        return date if as_datetime else date.strftime("%Y-%m-%d")

    hour_match = re.search(r"(\d+)\s*å°æ™‚å‰", date_str)
    if hour_match:
        date = now - timedelta(hours=int(hour_match.group(1)))
        return date if as_datetime else date.strftime("%Y-%m-%d")

    day_match = re.search(r"(\d+)\s*å¤©å‰", date_str)
    if day_match:
        date = now - timedelta(days=int(day_match.group(1)))
        return date if as_datetime else date.strftime("%Y-%m-%d")

    # ç„¡æ³•è§£æå°±å›å‚³ç¾åœ¨æ™‚é–“
    if as_datetime:
        return now # ç›´æ¥å›å‚³ timezone.now() ç‰©ä»¶
    else:
        return now.strftime("%Y-%m-%d") # å°‡ now ç‰©ä»¶æ ¼å¼åŒ–ç‚ºå­—ä¸²

def extract_tags(text, top_k=10, use_tfidf=True):
    """
    å¾ä¸€æ®µä¸­æ–‡æ–‡å­—ä¸­æ“·å–é—œéµå­—è©
    :param text: è¼¸å…¥çš„åŸå§‹æ–‡å­—
    :param top_k: æœ€å¤šæ“·å–å¹¾å€‹é—œéµå­—ï¼ˆåªæœ‰åœ¨ use_tfidf=True æ™‚ç”Ÿæ•ˆï¼‰
    :param use_tfidf: æ˜¯å¦ä½¿ç”¨ TF-IDF æ¬Šé‡é¸å­—ï¼ˆå¦å‰‡å°±æ˜¯ç´”åˆ†è©ï¼‰
    :param stopwords: åœç”¨è©æ¸…å–®ï¼ˆå¯ä»¥å®¢è£½ï¼‰
    :return: å­—è©æ¨™ç±¤çš„ list
    """
    stopwords = set([
      'çš„', 'äº†', 'æ˜¯', 'æˆ‘', 'ä½ ', 'ä»–', 'å¥¹', 'å®ƒ', 'æˆ‘å€‘', 'ä½ å€‘', 'ä»–å€‘', 'é€™', 'é‚£', 'å’Œ', 'èˆ‡',
      'åœ¨', 'ä¸', 'æœ‰', 'ä¹Ÿ', 'å°±', 'éƒ½', 'å¾ˆ', 'è€Œ', 'åŠ', 'æˆ–', 'è¢«', 'é‚„', 'èƒ½', 'æœƒ','æ ¸ç¨¿','ç·¨è¼¯',
      'å…§å®¹','è«‹è¦‹','ç™¼å¸ƒ','è¨‚é–±','é€²è¡Œ','æ ¹æ“š','å ±å°','æ–°è','é‡å°','ä¸€å','ç™¼ç¾','çµæœ','è¨˜å¾—',
    ])
    if use_tfidf:
        # ä½¿ç”¨ TF-IDF æŠ½å–é—œéµè©
        tags = jieba.analyse.extract_tags(text, topK=top_k)
    else:
        # åŸºæœ¬æ–·è©
        tags = jieba.lcut(text)

    # éæ¿¾æ¨™é»ã€ç©ºå­—å…ƒã€åœç”¨è©
    filtered_tags = []
    for word in tags:
        word = word.strip()
        if word and re.match(r'^[\u4e00-\u9fff]+$', word) and word not in stopwords:
            filtered_tags.append(word)

    return filtered_tags

# TVBS æ–°èçˆ¬èŸ²
def get_tvbs_news(keyword='', max_pages=20, days=7):
    results = []  # ç”¨ä¾†å„²å­˜æ‰€æœ‰ç¬¦åˆæ¢ä»¶çš„æ–°èè³‡æ–™    
    headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"}   # æ¨¡æ“¬ç€è¦½å™¨ï¼Œé¿å…è¢«æ“‹çˆ¬
    if not keyword.strip():
        keyword = 'æ–°è'  # è‹¥æœªè¼¸å…¥é—œéµå­—ï¼Œé è¨­ç”¨ã€Œæ–°èã€
        if keyword == 'æ–°è':
            days=5
    today = datetime.now()  # ç¾åœ¨çš„æ™‚é–“
    seven_days_ago = today - timedelta(days=days)  # å¹¾å¤©å‰çš„æ™‚é–“ï¼Œç”¨ä¾†éæ¿¾éèˆŠæ–°è
     
    for page in range(1, max_pages + 1):  # é æ•¸å¾ 1 åˆ° max_pages
        stop_crawling = False  # æ§åˆ¶æ˜¯å¦ä¸­æ–·çˆ¬å–ï¼ˆé‡åˆ°å¤ªèˆŠçš„æ–°èå°±ä¸­æ–·ï¼‰
        
        # æ§‹é€ æŸ¥è©¢é é¢çš„ URL
        url = f"https://news.tvbs.com.tw/news/searchresult/{keyword}/news/{page}"
        try:
            # ç™¼é€ GET è«‹æ±‚ï¼Œè¨­å®š timeout é¿å…ç¶²è·¯ç„¡åæ‡‰æ™‚å¡ä½
            response = requests.get(url, headers=headers, timeout=10)
            if response.status_code != 200:
                print(f"è·³éç¬¬ {page} é ï¼šå›æ‡‰éŒ¯èª¤ {response.status_code}")
                continue  # è‹¥éæˆåŠŸå›æ‡‰ï¼Œå°±è·³éé€™é 
        except Exception as e:
            print(f"é€£ç·šå¤±æ•—ï¼š{e}")
            continue  # è‹¥ç™¼ç”Ÿä¾‹å¤–ä¹Ÿè·³é

        # ä½¿ç”¨ BeautifulSoup è§£æ HTML
        html = BeautifulSoup(response.text, "html.parser")
        
        # æ‰¾åˆ°æ–°èæ¸…å–®
        article_list = html.find('main').find('div', class_='list').find_all('li')
        if not article_list:
            break  # å¦‚æœæ²’æœ‰æ–°èæ¢ç›®ï¼Œå°±çµæŸçˆ¬å–

        for article in article_list:
            # æ“·å–æ—¥æœŸå­—ä¸²
            time_tag = article.find('div', class_='time')
            date_str = time_tag.text.strip() if time_tag else ''

            # å°‡æ—¥æœŸå­—ä¸²è½‰ç‚º datetime ç‰©ä»¶ï¼Œä¾›ç¯©é¸ç”¨
            date_obj = parse_date(date_str, True)
            if not date_obj or date_obj < seven_days_ago:
                stop_crawling = True  # ç™¼ç¾å¤ªèˆŠæ–°èï¼Œè¨­åœæ­¢æ——æ¨™
                break  # çµæŸç›®å‰é é¢çš„æ–°èè™•ç†

            # è½‰æ›ç‚ºå­—ä¸²å½¢å¼ï¼Œå­˜å…¥çµæœä¸­
            date = parse_date(date_str)

            # æ“·å–æ–°èé€£çµ
            a_tag = article.find('a')
            if not a_tag:
                continue  # è‹¥æ‰¾ä¸åˆ°é€£çµå‰‡ç•¥é

            # æ“·å–æ–°èæ¨™é¡Œ
            title_tag = article.find('h2', class_='txt')
            title = title_tag.text.strip() if title_tag else ''

            # æ“·å–é€£çµ URL
            news_url = a_tag['href'] if a_tag.has_attr('href') else ''
            
            # æ“·å–æ‘˜è¦å…§å®¹
            summary_tag = article.find('div', class_='summary')
            summary = summary_tag.text.strip() if summary_tag else ''

            # æ“·å–æ¨™ç±¤åˆ—è¡¨ï¼ˆåŸå§‹æ˜¯å­—ä¸²æ ¼å¼ï¼‰
            tags_raw = a_tag.get('data-news_tag', '[]')
            tags = [tag.strip(" '") for tag in tags_raw.strip('[]').split(',')]

            # æ“·å–æ–°èé¡åˆ¥
            category_tag = article.find('div', class_='type').find('a')
            category = category_tag.text.strip() if category_tag else ''

            # åŠ å…¥çµæœåˆ—è¡¨
            results.append({
                'keyword': keyword,
                'title': title,
                'date': date,  
                'summary': summary,
                'news_tag': tags,
                'news_url': news_url,
                'category': category,
                'source': 'TVBSæ–°èç¶²',
            })

        if stop_crawling:
            break  # è‹¥è¨­å®šä¸­æ–·çˆ¬å–ï¼Œè·³å‡ºå¤–å±¤é æ•¸è¿´åœˆ

    return results  # å›å‚³æ‰€æœ‰çµæœ
# ä¸­æ™‚æ–°èçˆ¬èŸ²(è¢«æ“‹)
def get_chdtv_news(keyword, max_pages=3):
    results = []
    for page in range(1, max_pages + 1):
        url = f"https://www.chinatimes.com/search/{keyword}?page={page}&chdtv"
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"
        }
        # ç™¼é€ GET è«‹æ±‚
        response = requests.get(url, headers=headers)
        if response.status_code != 200:
            continue
        html = BeautifulSoup(response.text, "html.parser")

        # æŠ“å–æ–°èå€å¡Š
        article_list = html.find('div', class_='wrapper').find('ul', class_='vertical-list list-style-none').find_all('li')
        for article in article_list:
            a_tag = article.find('a')
            if not a_tag:
                continue    
            # æ¨™é¡Œ
            title_tag = article.find('h3', class_='title')
            title = title_tag.text.strip() if title_tag else ''

            # æ–°èé€£çµ
            news_url = a_tag['href'] if a_tag.has_attr('href') else ''

            # ç™¼å¸ƒæ™‚é–“
            time_tag = article.find('span', class_='date')
            date = time_tag.text.strip() if time_tag else ''

            # æ‘˜è¦
            summary_tag = article.find('p', class_='intro')
            summary = summary_tag.text.strip() if summary_tag else ''

            # æ¨™ç±¤
            tags = extract_tags(summary)

            # åŠ å…¥çµæœ
            results.append({
                'title': title,
                'date': parse_date(date),
                'summary': summary,
                'news_tag': tags,
                'news_url': news_url,
                'source':'ä¸­æ™‚æ–°èç¶²',
            })
    return results
# è‡ªç”±æ™‚å ±æ–°èçˆ¬èŸ²
def get_LTN_news(keyword='', max_pages=25, days=7):
    results = []

    if not keyword.strip():
        keyword = 'æ–°è'
        if keyword == 'æ–°è':
            days=5
    stop_crawling = False
    headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"}

    end_date = datetime.today()
    start_date = end_date - timedelta(days=days)
    end_time = end_date.strftime('%Y%m%d')
    start_time = start_date.strftime('%Y%m%d')
    for page in range(1, max_pages + 1):
        url = f'https://search.ltn.com.tw/list?keyword={keyword}&start_time={start_time}&end_time={end_time}&sort=date&type=all&page={page}'
        
        try:
            response = requests.get(url, headers=headers, timeout=10)
            if response.status_code != 200:
                print(f"è·³éç¬¬ {page} é ï¼ŒHTTP éŒ¯èª¤ï¼š{response.status_code}")
                continue
        except Exception as e:
            print(f"é€£ç·šéŒ¯èª¤ï¼š{e}")
            continue
        html = BeautifulSoup(response.text, 'html.parser')
        # print(html)

        # æŠ“å–æ–°èå€å¡Š
        article_list = html.find('section',class_='Searchnews').find('div',class_='page-name').find_all('li')

        for article in article_list:
            # ç™¼å¸ƒæ™‚é–“
            time_tag = article.find('span', class_='time')
            date_str = time_tag.text.strip() if time_tag else ''
            date_init = parse_date(date_str,True)
            if not date_init or date_init < start_date:
                stop_crawling = True
                break
            date = parse_date(date_str)
            
            a_tag = article.find('a')
            if not a_tag:
                continue    
        
            # æ‘˜è¦
            summary_tag = article.find('p')
            summary = summary_tag.text.strip() if summary_tag else ''

            # æ¨™ç±¤
            tags = extract_tags(summary)
            if not tags:  
                continue
            # æ¨™é¡Œ
            title = a_tag['title'] if a_tag.has_attr('title') else ''

            # æ–°èé€£çµ
            news_url = a_tag['href'] if a_tag.has_attr('href') else ''

            # é¡åˆ¥
            category_tag = article.find('i')
            category = category_tag.text.strip() if category_tag else ''

            # åŠ å…¥çµæœ
            results.append({
                'keyword': keyword,
                'title': title,
                'date': date,
                'summary': summary,
                'news_tag': tags,
                'news_url': news_url,
                'category': category,
                'source':'è‡ªç”±æ™‚å ±',
            })
        if stop_crawling:
            break
    return results
# ETtodayæ–°èçˆ¬èŸ²
def get_ET_news(keyword='', max_pages=30, days=7):
    results = []
    if not keyword.strip():
        keyword = 'æ–°è'
        if keyword == 'æ–°è':
            days=5
    stop_crawling = False
    headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"}

    today = datetime.now()  # ç¾åœ¨çš„æ™‚é–“
    days_ago = today - timedelta(days=days)  # å¹¾å¤©å‰çš„æ™‚é–“ï¼Œç”¨ä¾†éæ¿¾éèˆŠæ–°è

    for page in range(1, max_pages + 1):
        url = f"https://www.ettoday.net/news_search/doSearch.php?keywords={keyword}&idx=1&page={page}"
        try:
            response = requests.get(url, headers=headers, timeout=10)
            if response.status_code != 200:
                print(f"è·³éç¬¬ {page} é ï¼ŒHTTP éŒ¯èª¤ï¼š{response.status_code}")
                continue
        except Exception as e:
            print(f"é€£ç·šéŒ¯èª¤ï¼š{e}")
            continue

        html = BeautifulSoup(response.text, "html.parser")

        # æ ¹æ“šå¯¦éš›ç¶²é çµæ§‹å®šä½æ–‡ç« å€å¡Š
        article_list = html.select("div.archive.clearfix")

        for article in article_list:
            # ç™¼å¸ƒæ™‚é–“
            time_tag = article.select_one('.date')
            date_str = time_tag.text.strip() if time_tag else ''
            date_init = parse_date(date_str,True)
            if not date_init or date_init < days_ago:
                stop_crawling = True
                break
            date = parse_date(date_str)

            a_tag = article.find("a")
            if not a_tag:
                continue

            # æ¨™é¡Œ
            title_tag = article.find("h2")
            title = title_tag.text.strip() if title_tag else ""

            # æ–°èé€£çµ
            news_url = a_tag["href"] if a_tag.has_attr("href") else ""

            # æ‘˜è¦
            summary_tag = article.find("p")
            summary = summary_tag.text.strip() if summary_tag else ""

            # æ¨™ç±¤
            tags = extract_tags(summary)

            # é¡åˆ¥
            category_tag = article.find('span', class_='date').find('a')
            category = category_tag.text.strip() if category_tag else ''

            results.append({
                "keyword": keyword,
                "title": title,
                "date": date,
                "summary": summary,
                "news_tag": tags,
                "news_url": news_url,
                "category": category,
                "source": "ETtodayæ–°èé›²",
            })
        if stop_crawling:
            break
    return results
# æ•´åˆæ–°èæ–‡ç« 
def search_news(keyword):
    articles = (
        get_tvbs_news(keyword) +
        get_ET_news(keyword) +
        get_LTN_news(keyword)
    )
    return articles
# ä½¿ç”¨ SnowNLP åˆ†æå­—ä¸²æƒ…ç·’
def analyze_sentiment(articles):
    """
    :param articles: æ¯ç¯‡æ–‡ç« ç‚º dictï¼Œéœ€åŒ…å« summary å’Œ title
    :return: å›å‚³åŸå§‹é™£åˆ—ï¼Œæ¯ç¯‡æ–‡ç« åŠ å…¥ sentiment_score åŠ sentiment æ¨™ç±¤ï¼ˆæ­£é¢ / è² é¢ / ä¸­ç«‹ï¼‰
    """
    if not articles:  # é˜²æ­¢ None æˆ–ç©ºå€¼
        return []

    for article in articles:
        # å…ˆä½¿ç”¨ summaryï¼Œè‹¥ç‚ºç©ºå‰‡ç”¨ title
        text = article['summary'] if article.get('summary') else article.get('title', '')
        
        s = SnowNLP(text)
        score = s.sentiments  # åˆ†æ•¸ä»‹æ–¼ 0~1ï¼Œæ„ˆæ¥è¿‘ 1 è¶Šæ­£é¢ï¼Œæ¥è¿‘ 0 è¶Šè² é¢
        article['sentiment_score'] = score

        # åŠ å…¥ä¸­ç«‹åˆ¤æ–·é‚è¼¯ï¼š0~0.4 è² é¢ã€0.4~0.6 ä¸­ç«‹ã€0.6~1 æ­£é¢
        if score >= 0.6:
            article['sentiment'] = 'æ­£é¢'   # æ­£é¢
        elif score <= 0.4:
            article['sentiment'] = 'è² é¢'  # è² é¢
        else:
            article['sentiment'] = 'ä¸­ç«‹'   # ä¸­ç«‹

    return articles
# è¨ˆç®—æƒ…ç·’åˆ†é¡æ¬¡æ•¸
def count_sentiment(articles):
    sentiment_count = {
        'æ­£é¢': sum(1 for a in articles if a['sentiment'] == 'æ­£é¢'),
        'è² é¢': sum(1 for a in articles if a['sentiment'] == 'è² é¢'),
        'ä¸­ç«‹': sum(1 for a in articles if a['sentiment'] == 'ä¸­ç«‹'),
    }
    return sentiment_count
# æ ¹æ“šæƒ…ç·’æ¨™ç±¤çµ±è¨ˆæ­£é¢ã€è² é¢ã€ä¸­ç«‹æ‘˜è¦ä¸­çš„é«˜é »è©
def get_top_words(articles, top_n=5):
    """
    :param articles: åŒ…å« 'summary' èˆ‡ 'sentiment' æ¬„ä½çš„æ–‡ç« åˆ—è¡¨
    :param top_n: æ¯å€‹æƒ…ç·’é¡åˆ¥ä¸­é¡¯ç¤ºçš„å‰ N åé«˜é »è©
    :return: dictï¼ŒåŒ…å«ä¸‰é¡è©å½™çš„ top_n çµæœ
    """
    pos_words, neg_words, neu_words = [], [], []

    for article in articles:
        words = extract_tags(article['summary'])
        if article['sentiment'] == 'æ­£é¢':
            pos_words.extend(words)
        elif article['sentiment'] == 'è² é¢':
            neg_words.extend(words)
        elif article['sentiment'] == 'ä¸­ç«‹':
            neu_words.extend(words)

    return {
        'positive': Counter(pos_words).most_common(top_n),
        'negative': Counter(neg_words).most_common(top_n),
        'neutral': Counter(neu_words).most_common(top_n)
    }
# è¨ˆç®—æƒ…ç·’å‡ºç¾é »ç‡
def sentiment_feq(data,col):
    stats = defaultdict(lambda: {'positive': 0, 'neutral': 0, 'negative': 0})
    for d in data:
        column = d[col]
        if d['sentiment'] == 'æ­£é¢':
            stats[column]['positive'] += 1
        elif d['sentiment'] == 'ä¸­ç«‹':
            stats[column]['neutral'] += 1
        elif d['sentiment'] == 'è² é¢':
            stats[column]['negative'] += 1
    return dict(stats)
# ç”Ÿæˆé«˜é »å­—æ–‡å­—é›²
def generate_wordcloud(tags, save_path):
    import platform

    # æ ¹æ“šç³»çµ±è‡ªå‹•é¸æ“‡å­—é«”
    system = platform.system()
    if system == "Windows":
        font_path = "C:/Windows/Fonts/msjh.ttc"  # å¾®è»Ÿæ­£é»‘é«”
    elif system == "Darwin":
        font_path = "/System/Library/Fonts/STHeiti Medium.ttc"  # macOS é»‘é«”
    else:
        font_path = "/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf"  # Linux å¸¸è¦‹å­—é«”

    if not os.path.exists(font_path):
        raise OSError(f"âš ï¸ æ‰¾ä¸åˆ°å­—é«”æª”æ¡ˆï¼š{font_path}")

    # è£½ä½œæ–‡å­—é›²
    text = ' '.join(tags)
    wc = WordCloud(
        background_color="white", 
        font_path=font_path, 
        width=1096, 
        height=480,
        max_words=100,
    )
    wc.generate(text)
    wc.to_file(save_path)
# è¨ˆç®—æ™‚é–“åºåˆ—æ–°èæ•¸é‡
def news_post_counter(articles):
    # å°‡è³‡æ–™æ•´ç†æˆæ¯æ—¥æ•¸é‡ dict
    daily_counts = Counter()
    for article in articles:
        if article['date']:
            daily_counts[article['date']] += 1

    # è½‰ç‚ºæ’åºå¾Œçš„ x, y list
    trend_labels = sorted(daily_counts.keys())
    trend_values = [daily_counts[date] for date in trend_labels]
    return trend_labels,trend_values
# AIåˆ†æå ±å‘Špromptç”Ÿæˆ
def generate_prompt(keyword, sentiment_count, top_words, sentiment_by_catORsour):
    prompt = f"""
    è«‹æ ¹æ“šä»¥ä¸‹è³‡æ–™(æ–°èæˆ–ç¤¾ç¾¤è²¼æ–‡)ï¼Œæ’°å¯«ä¸€ç¯‡åˆ†æå ±å‘Šï¼Œå­—æ•¸ç´„ 300 å­—ï¼Œä»¥æ•¸æ“šåˆ†æå¸«çš„è§’åº¦å»åˆ†æï¼Œèªæ°£å°ˆæ¥­æ¸…æ¥šï¼Œ
    æ¢åˆ—å¼è¡¨é”ï¼Œç›´æ¥çµ¦åˆ†æçµæœå°±å¥½ï¼š
    
    ğŸ” ä¸»é¡Œï¼š{keyword}

    ğŸ“Š æƒ…ç·’æ¯”ä¾‹ï¼š
    æ­£é¢æ–‡ç« æ•¸ï¼š{sentiment_count.get('æ­£é¢', 0)}
    è² é¢æ–‡ç« æ•¸ï¼š{sentiment_count.get('è² é¢', 0)}
    ä¸­ç«‹æ–‡ç« æ•¸ï¼š{sentiment_count.get('ä¸­ç«‹', 0)}

    ğŸ“Œ é¡åˆ¥æƒ…ç·’çµ±è¨ˆæ¦‚æ³æˆ–è²¼æ–‡ä¾†æºæƒ…ç·’çµ±è¨ˆæ¦‚æ³(è«‹è‡ªè¡Œåˆ¤æ–·ï¼‰ï¼š
    {sentiment_by_catORsour}

    ğŸ”¥ é«˜é »é—œéµè©(æ¯å€‹è©åœ¨æ–‡ç« å‡ºç¾çš„æ•¸é‡):
    {"ã€".join(top_words.get('all', []))}

    è«‹ç¶œåˆä»¥ä¸Šè³‡è¨Šï¼Œèªªæ˜ç›®å‰ç†±åº¦è¶¨å‹¢èˆ‡ç¤¾ç¾¤é—œæ³¨é‡é»ã€‚
    """
    return prompt
# è¼‰å…¥å¤§èªè¨€æ¨¡å‹Gemini
def call_LLM(prompt, api_key):
    genai.configure(api_key = api_key)
    model = genai.GenerativeModel('gemini-1.5-flash') # gemini-1.5-pro/ gemini-1.5-flash
    response = model.generate_content(prompt)
    return response.text.strip()
# åŸ·è¡Œæ–°èæ‰€æœ‰æµç¨‹
def news_work(keyword, api_key):
    start_time = datetime.now().strftime("%Y%m%d_%H%M")
    # 1. æœå°‹èˆ‡æƒ…ç·’åˆ†æ
    articles = analyze_sentiment(search_news(keyword))
    # 2. è¨ˆç®—æ­£è² æƒ…ç·’æ•¸é‡
    sentiment_count = count_sentiment(articles)
    # 3. è¶¨å‹¢åˆ†æï¼ˆå„æ™‚é–“é»çš„æ–°èæ•¸é‡ï¼‰
    trend_labels,trend_values = news_post_counter(articles)
    # 4. åˆ†æè©å½™è²¢ç»
    top_word = get_top_words(articles)
    # 5. åˆ†æåˆ†é¡æƒ…ç·’
    category_stats = sentiment_feq(articles,'category')
    # 6. çµ±è¨ˆæ¨™ç±¤è©å½™è£½ä½œæ–‡å­—é›²åœ–
    all_tags = []
    for art in articles:
        all_tags.extend(art['news_tag'])
    
    wordcloud_path = os.path.join(BASE_DIR, 'static', 'clouds', f'n-{keyword}{start_time}.png')
    os.makedirs(os.path.dirname(wordcloud_path), exist_ok=True)
    generate_wordcloud(all_tags, wordcloud_path)
    # 7. ä½¿ç”¨ Gemini ç”Ÿæˆå ±å‘Š
    prompt = generate_prompt(keyword, sentiment_count, top_word, category_stats)
    try:
        report = call_LLM(prompt, api_key)
    except Exception as e:
        report = f"âš ï¸ Gemini å›æ‡‰å¤±æ•—ï¼š{e}"

    end_time = datetime.now().strftime("%Y%m%d_%H%M")
    # å›å‚³çµæœ
    analysis = {
        'positive_count': sentiment_count['æ­£é¢'],
        'negative_count': sentiment_count['è² é¢'],
        'neutral_count': sentiment_count['ä¸­ç«‹'],
        'cate_count':  category_stats,
        'tag_image': f'./static/clouds/n-{keyword}{start_time}.png',
        'top_word': top_word,
        'trend_labels': trend_labels,
        'trend_values': trend_values,
        'report':report,
    }
    return articles, analysis

def posts_work(keyword, api_key):
    start_time = datetime.now().strftime("%Y%m%d_%H%M")
    ptt_posts = analyze_sentiment(get_ptt_posts())
    threads_posts = analyze_sentiment(scrape_threads_by_keyword(keyword))
    if keyword.strip():
        ptt_posts = ptt_keyword(keyword,ptt_posts)
    posts = ptt_posts + threads_posts
    sentiment_count = count_sentiment(posts)
    trend_labels,trend_values = news_post_counter(posts)
    top_word = get_top_words(posts)
    source_stats = sentiment_feq(posts,'source')
    all_tags = []
    for post in posts:
        all_tags.extend(extract_tags(post['summary']))
    print('tags:',len(all_tags))
    wordcloud_path = os.path.join(BASE_DIR, 'static', 'clouds', f'p-{keyword}{start_time}.png')
    os.makedirs(os.path.dirname(wordcloud_path), exist_ok=True)
    generate_wordcloud(all_tags, wordcloud_path)
    # 7. ä½¿ç”¨ Gemini ç”Ÿæˆå ±å‘Š
    prompt = generate_prompt(keyword, sentiment_count, top_word, source_stats)
    try:
        report = call_LLM(prompt, api_key)
    except Exception as e:
        report = f"âš ï¸ Gemini å›æ‡‰å¤±æ•—ï¼š{e}"
    # å›å‚³çµæœ
    analysis = {
        'pos_count': sentiment_count['æ­£é¢'],
        'neg_count': sentiment_count['è² é¢'],
        'neu_count': sentiment_count['ä¸­ç«‹'],
        'sour_count':  source_stats,
        'post_image': f'./static/clouds/p-{keyword}{start_time}.png',
        'post_top_word': top_word,
        'post_trend_labels': trend_labels,
        'post_trend_values': trend_values,
        'post_report':report,
    }
    return posts, analysis
    

    
# source venv/bin/activate
# python manage.py runserver