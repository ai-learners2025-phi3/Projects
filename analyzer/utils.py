import time, random, requests
from bs4 import BeautifulSoup
import matplotlib.pyplot as plt
import platform
import os
from datetime import datetime, timedelta
import re
import jieba
import jieba.analyse
from collections import Counter, defaultdict
import pandas as pd
from snownlp import SnowNLP
from wordcloud import WordCloud
import google.generativeai as genai
from dotenv import load_dotenv

import pytz

tz = pytz.timezone("Asia/Taipei")


BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))


def get_latest_articles_by_source(articles, top_n=5):
    """
    å°‡ articles æŒ‰ source åˆ†çµ„ï¼Œå„å–å‰ top_n ç­†ï¼Œä¸¦ç¶­æŒåŸé †åºåˆä½µå›ä¸€å€‹åˆ—è¡¨ã€‚
    :param articles: List[dict]ï¼Œæ¯ç­†è¦æœ‰ 'source' æ¬„ä½
    :param top_n: intï¼Œå„ä¾†æºè¦å–çš„æ•¸é‡
    :return: List[dict]
    """
    grouped = defaultdict(list)
    for art in articles:
        src = art.get('source', 'Unknown')
        grouped[src].append(art)

    # æŠŠæ¯çµ„çš„å‰ top_n ç­†æ‹¼å›ä¸€å€‹åˆ—è¡¨
    result = []
    for src, arts in grouped.items():
        result.extend(arts[:top_n])
    return result

def parse_date(date_str):
    date_str = date_str.strip()
    now = datetime.now()

    # å¾æ··äº‚å­—ä¸²ä¸­æŠ½å– YYYY-MM-DD æ ¼å¼
    match = re.search(r'\d{4}-\d{2}-\d{2}', date_str)
    if match:
        return match.group()

    # æ”¯æ´å¤šç¨®æ ¼å¼è§£æ
    date_formats = [
        "%Y/%m/%d",
        "%Y/%m/%d %H:%M",
        "%Y-%m-%d",
        "%Yå¹´%mæœˆ%dæ—¥",
    ]
    for fmt in date_formats:
        try:
            date = datetime.strptime(date_str, fmt)
            return date.strftime("%Y-%m-%d")
        except Exception:
            continue

    # ç›¸å°æ™‚é–“è™•ç†
    if "å‰›å‰›" in date_str or "ç§’å‰" in date_str:
        return now.strftime("%Y-%m-%d")
    
    minute_match = re.search(r"(\d+)\s*åˆ†é˜å‰", date_str)
    if minute_match:
        date = now - timedelta(minutes=int(minute_match.group(1)))
        return date.strftime("%Y-%m-%d")
    
    hour_match = re.search(r"(\d+)\s*å°æ™‚å‰", date_str)
    if hour_match:
        date = now - timedelta(hours=int(hour_match.group(1)))
        return date.strftime("%Y-%m-%d")

    day_match = re.search(r"(\d+)\s*å¤©å‰", date_str)
    if day_match:
        date = now - timedelta(days=int(day_match.group(1)))
        return date.strftime("%Y-%m-%d")

    # ç„¡æ³•è§£æå°±åŸæ¨£å›å‚³
    return date_str

def extract_tags(text, top_k=10, use_tfidf=True):
    """
    å¾ä¸€æ®µä¸­æ–‡æ–‡å­—ä¸­æ“·å–é—œéµå­—è©
    :param text: è¼¸å…¥çš„åŸå§‹æ–‡å­—
    :param top_k: æœ€å¤šæ“·å–å¹¾å€‹é—œéµå­—ï¼ˆåªæœ‰åœ¨ use_tfidf=True æ™‚ç”Ÿæ•ˆï¼‰
    :param use_tfidf: æ˜¯å¦ä½¿ç”¨ TF-IDF æ¬Šé‡é¸å­—ï¼ˆå¦å‰‡å°±æ˜¯ç´”åˆ†è©ï¼‰
    :param stopwords: åœç”¨è©æ¸…å–®ï¼ˆå¯ä»¥å®¢è£½ï¼‰
    :return: å­—è©æ¨™ç±¤çš„ list
    """
    stopwords = set([
      'çš„', 'äº†', 'æ˜¯', 'æˆ‘', 'ä½ ', 'ä»–', 'å¥¹', 'å®ƒ', 'æˆ‘å€‘', 'ä½ å€‘', 'ä»–å€‘', 'é€™', 'é‚£', 'å’Œ', 'èˆ‡',
      'åœ¨', 'ä¸', 'æœ‰', 'ä¹Ÿ', 'å°±', 'éƒ½', 'å¾ˆ', 'è€Œ', 'åŠ', 'æˆ–', 'è¢«', 'é‚„', 'èƒ½', 'æœƒ',
    ])
    if use_tfidf:
        # ä½¿ç”¨ TF-IDF æŠ½å–é—œéµè©
        tags = jieba.analyse.extract_tags(text, topK=top_k)
    else:
        # åŸºæœ¬æ–·è©
        tags = jieba.lcut(text)

    # éæ¿¾æ¨™é»ã€ç©ºå­—å…ƒã€åœç”¨è©
    filtered_tags = []
    for word in tags:
        word = word.strip()
        if word and re.match(r'^[\u4e00-\u9fff]+$', word) and word not in stopwords:
            filtered_tags.append(word)

    return filtered_tags

def get_tvbs_news(keyword, max_pages=4):
    results = []

    for page in range(1, max_pages + 1):
        url = f"https://news.tvbs.com.tw/news/searchresult/{keyword}/news/{page}"
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"
        }
        # ç™¼é€ GET è«‹æ±‚
        response = requests.get(url, headers=headers)
        if response.status_code != 200:
            continue
        html = BeautifulSoup(response.text, "html.parser")

        # æŠ“å–æ–°èå€å¡Š
        article_list = html.find('main').find('div', class_='list').find_all('li')
        for article in article_list:
            a_tag = article.find('a')
            if not a_tag:
                continue    
            # æ¨™é¡Œ
            title_tag = article.find('h2', class_='txt')
            title = title_tag.text.strip() if title_tag else ''

            # æ–°èé€£çµ
            news_url = a_tag['href'] if a_tag.has_attr('href') else ''

            # ç™¼å¸ƒæ™‚é–“
            time_tag = article.find('div', class_='time')
            date = time_tag.text.strip() if time_tag else ''

            # æ‘˜è¦
            summary_tag = article.find('div', class_='summary')
            summary = summary_tag.text.strip() if summary_tag else ''

            # æ¨™ç±¤
            tags_raw = a_tag.get('data-news_tag', '[]')
            # è™•ç†æˆä¹¾æ·¨çš„ listï¼ˆç§»é™¤ ' èˆ‡ç©ºæ ¼ï¼‰
            tags = [tag.strip(" '") for tag in tags_raw.strip('[]').split(',')]

            # é¡åˆ¥
            category_tag = article.find('div', class_='type').find('a')
            category = category_tag.text.strip() if category_tag else ''

            # åŠ å…¥çµæœ
            results.append({
                'title': title,
                'date': parse_date(date),
                'summary': summary,
                'news_tag': tags,
                'news_url': news_url,
                'category': category,
                'source':'TVBSæ–°èç¶²',
            })
    return results

def get_chdtv_news(keyword, max_pages=3):
    results = []
    for page in range(1, max_pages + 1):
        url = f"https://www.chinatimes.com/search/{keyword}?page={page}&chdtv"
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"
        }
        # ç™¼é€ GET è«‹æ±‚
        response = requests.get(url, headers=headers)
        if response.status_code != 200:
            continue
        html = BeautifulSoup(response.text, "html.parser")

        # æŠ“å–æ–°èå€å¡Š
        article_list = html.find('div', class_='wrapper').find('ul', class_='vertical-list list-style-none').find_all('li')
        for article in article_list:
            a_tag = article.find('a')
            if not a_tag:
                continue    
            # æ¨™é¡Œ
            title_tag = article.find('h3', class_='title')
            title = title_tag.text.strip() if title_tag else ''

            # æ–°èé€£çµ
            news_url = a_tag['href'] if a_tag.has_attr('href') else ''

            # ç™¼å¸ƒæ™‚é–“
            time_tag = article.find('span', class_='date')
            date = time_tag.text.strip() if time_tag else ''

            # æ‘˜è¦
            summary_tag = article.find('p', class_='intro')
            summary = summary_tag.text.strip() if summary_tag else ''

            # æ¨™ç±¤
            tags = extract_tags(summary)

            # åŠ å…¥çµæœ
            results.append({
                'title': title,
                'date': parse_date(date),
                'summary': summary,
                'news_tag': tags,
                'news_url': news_url,
                'source':'ä¸­æ™‚æ–°èç¶²',
            })
    return results

def get_now_news(keyword, max_pages=5):
    results = []
    for page in range(1, max_pages + 1):
        url = f"https://www.nownews.com/search?q={keyword}&page={page}"
        headers = {"User-Agent": "Mozilla/5.0"}
        resp = requests.get(url, headers=headers, timeout=5)
        if resp.status_code != 200:
            continue

        html = BeautifulSoup(resp.text, "html.parser")

        main_blk = html.find('div', class_='mainBlk')
        if not main_blk:
            continue

        item_list = main_blk.find('div', class_='item-list')
        if not item_list:
            continue

        article_list = item_list.find_all('a')
        if not article_list:
            continue

        for article in article_list:
            title_tag = article.find('h3', class_='title')
            title = title_tag.get_text(strip=True) if title_tag else ''

            news_url = article.get('href', '')

            time_el = article.find('p', class_='time')
            date = time_el.find('time').get_text(strip=True) if time_el and time_el.find('time') else ''

            # æ­£ç¢ºåŒ¹é…åŒæ™‚å«æœ‰ content èˆ‡ text-truncate çš„ <p>
            summary_el = article.select_one('p.content.text-truncate')
            summary = summary_el.get_text(strip=True) if summary_el else ''

            if not title and not summary:
                continue

            tags = extract_tags(summary)

            results.append({
                'title': title,
                'date': parse_date(date),
                'summary': summary,
                'news_tag': tags,
                'news_url': news_url,
                'source':'NOWnews',
            })
    return results


def get_ettoday_news(keyword, start_date=None, end_date=None, max_pages=5):
    results = []
    
    for page in range(1, max_pages+1):
        url = f"https://www.ettoday.net/news_search/doSearch.php?keywords={keyword}&idx=1&page={page}"
        r = requests.get(url, headers={"User-Agent":"Mozilla/5.0"})
        if r.status_code != 200:
            continue
        soup = BeautifulSoup(r.text, "html.parser")
        for art in soup.select("div.archive.clearfix"):
            # å‹•æ…‹åŒ¯å…¥ï¼Œé¿å…å¾ªç’°ä¾è³´
            from analyzer.utils import parse_date

            link    = art.find("a")
            title   = art.find("h2").get_text(strip=True)
            href    = link["href"]
            date    = art.select_one(".date").get_text(strip=True)
            summary = art.find("p").get_text(strip=True)

            # è§£æä¸¦ç¯©é¸æ—¥æœŸ
            dt = parse_date(date)
            if start_date and dt < start_date:   continue
            if end_date   and dt > end_date:     continue

            results.append({
                "title": title,
                "date": dt,
                "summary": summary,
                "news_url": href,
                "source": "ETtodayæ–°èé›²",
            })
        time.sleep(1)
    return results

def get_ltn_news(keyword, max_pages=3):
    base = "https://talk.ltn.com.tw/search"
    results = []
    
    for page in range(1, max_pages+1):
        params = {"keyword":keyword, "page":page}
        r = requests.get(base, params=params, headers={"User-Agent":"Mozilla/5.0"})
        soup = BeautifulSoup(r.text, "html.parser")
        for art in soup.select(".searchlistbox .searchlistitem"):
            a = art.find("h3").find("a")
            title = a.get_text(strip=True)
            href  = a["href"]
            date_s = art.select_one(".searchlistinfo").get_text(strip=True)
            # è§£ææ™‚é–“
            try:
                dt = datetime.strptime(date_s, "%Y-%m-%d %H:%M").astimezone(tz).strftime("%Y-%m-%d")
            except:
                dt = date_s
            summary = art.find("p").get_text(strip=True) if art.find("p") else ""
            if keyword.lower() not in (title+summary).lower():
                continue
            results.append({
                "title": title,
                "date": dt,
                "summary": summary,
                "news_url": href,
                "source": "è‡ªç”±æ™‚å ±",
            })
        time.sleep(random.uniform(0.5,1.2))
    return results

HEADERS = {
    "User-Agent":"Mozilla/5.0 (Windows NT 10.0; Win64; x64)"
}

def get_udn_news(keyword, page_num=3):
    results = []
    
    for p in range(1, page_num+1):
        api = f"https://udn.com/api/more?page={p}&channelId=1&cate_id=0&type=breaknews"
        resp = requests.get(api, headers=HEADERS)
        data = resp.json().get("lists", [])
        for item in data:
            title = item.get("title","").strip()
            url   = "https://udn.com" + item.get("url","")
            ts    = item.get("timestamp",0)
            date  = datetime.fromtimestamp(ts).strftime("%Y-%m-%d")
            snippet = item.get("excerpt","").strip()
            # é—œéµå­—éæ¿¾
            if keyword.lower() not in (title+snippet).lower():
                continue
            results.append({
                "title": title,
                "date": date,
                "summary": snippet,
                "news_url": url,
                "source": "è¯åˆæ–°èç¶²",
            })
        time.sleep(random.uniform(0.5, 1.5))
    return results

def search_news(keyword):
    tvbs = get_tvbs_news(keyword)
    et   = get_ettoday_news(keyword)
    udn  = get_udn_news(keyword)
    ltn  = get_ltn_news(keyword)
    now  = get_now_news(keyword)
    chd  = get_chdtv_news(keyword)
    # ä½ ä¹Ÿå¯ä»¥åˆä½µå…¶å®ƒä¾†æº
    return tvbs + et + udn + ltn + now + chd

def analyze_sentiment(articles):
    """
    ä½¿ç”¨ SnowNLP åˆ†ææƒ…ç·’
    :param articles: æ¯ç¯‡æ–‡ç« ç‚º dictï¼Œéœ€åŒ…å« summary å’Œ title
    :return: å›å‚³åŸå§‹é™£åˆ—ï¼Œæ¯ç¯‡æ–‡ç« åŠ å…¥ sentiment_score åŠ sentiment æ¨™ç±¤ï¼ˆæ­£é¢ / è² é¢ / ä¸­ç«‹ï¼‰
    """
    for article in articles:
        # å…ˆä½¿ç”¨ summaryï¼Œè‹¥ç‚ºç©ºå‰‡ç”¨ title
        text = article['summary'] if article.get('summary') else article.get('title', '')
        
        s = SnowNLP(text)
        score = s.sentiments  # åˆ†æ•¸ä»‹æ–¼ 0~1ï¼Œæ„ˆæ¥è¿‘ 1 è¶Šæ­£é¢ï¼Œæ¥è¿‘ 0 è¶Šè² é¢
        article['sentiment_score'] = score

        # åŠ å…¥ä¸­ç«‹åˆ¤æ–·é‚è¼¯ï¼š0~0.4 è² é¢ã€0.4~0.6 ä¸­ç«‹ã€0.6~1 æ­£é¢
        if score >= 0.6:
            article['sentiment'] = 'æ­£é¢'   # æ­£é¢
        elif score <= 0.4:
            article['sentiment'] = 'è² é¢'  # è² é¢
        else:
            article['sentiment'] = 'ä¸­ç«‹'   # ä¸­ç«‹

    return articles

def count_sentiment(articles):
    sentiment_count = {
        'æ­£é¢': sum(1 for a in articles if a['sentiment'] == 'æ­£é¢'),
        'è² é¢': sum(1 for a in articles if a['sentiment'] == 'è² é¢'),
        'ä¸­ç«‹': sum(1 for a in articles if a['sentiment'] == 'ä¸­ç«‹'),
    }
    return sentiment_count

def get_top_words(articles, top_n=5):
    """
    æ ¹æ“šæ–‡ç« æƒ…ç·’æ¨™ç±¤ï¼Œçµ±è¨ˆæ­£é¢ã€è² é¢ã€ä¸­ç«‹æ‘˜è¦ä¸­çš„é«˜é »è©ã€‚
    
    :param articles: åŒ…å« 'summary' èˆ‡ 'sentiment' æ¬„ä½çš„æ–‡ç« åˆ—è¡¨
    :param top_n: æ¯å€‹æƒ…ç·’é¡åˆ¥ä¸­é¡¯ç¤ºçš„å‰ N åé«˜é »è©
    :return: dictï¼ŒåŒ…å«ä¸‰é¡è©å½™çš„ top_n çµæœ
    """
    pos_words, neg_words, neu_words = [], [], []

    for article in articles:
        words = extract_tags(article['summary'])
        if article['sentiment'] == 'æ­£é¢':
            pos_words.extend(words)
        elif article['sentiment'] == 'è² é¢':
            neg_words.extend(words)
        elif article['sentiment'] == 'ä¸­ç«‹':
            neu_words.extend(words)

    return {
        'positive': Counter(pos_words).most_common(top_n),
        'negative': Counter(neg_words).most_common(top_n),
        'neutral': Counter(neu_words).most_common(top_n)
    }

def sentiment_by_category(articles):
    """
    çµ±è¨ˆå„åˆ†é¡ï¼ˆcategoryï¼‰ä¸‹çš„æƒ…ç·’æ•¸é‡ï¼Œ
    è‹¥ article æ²’æœ‰ categoryï¼Œå°±æ”¹ç”¨ source ç•¶åˆ†é¡ã€‚
    """
    stats = defaultdict(lambda: {'positive': 0, 'neutral': 0, 'negative': 0})
    for art in articles:
        # è‹¥æ²’æœ‰ categoryï¼Œå°±æ‹¿ sourceï¼›è‹¥é€£ source ä¹Ÿæ²’æœ‰ï¼Œå°±æ¨™ç‚ºã€Œå…¶ä»–ã€
        cate = art.get('category', art.get('source', 'å…¶ä»–'))

        sentiment = art.get('sentiment')
        if sentiment == 'æ­£é¢':
            stats[cate]['positive'] += 1
        elif sentiment == 'ä¸­ç«‹':
            stats[cate]['neutral'] += 1
        elif sentiment == 'è² é¢':
            stats[cate]['negative'] += 1
        # å…¶ä»–æˆ–æœªæ¨™è¨˜å°±è·³é

    return dict(stats)


def random_color_func(word, font_size, position, orientation, random_state=None, **kwargs):
    """è‡ªè¨‚æ–‡å­—é›²æ–‡å­—é¡è‰²èª¿è‰²ç›¤"""
    palette = ["#cceaff", "#7cf8dd", "#faa7a8", "#ffca95", "#dbc2f7", "#ffd9c4"]
    return random.choice(palette)

def generate_wordcloud(frequencies, save_path, max_words=50, min_font_size=20):
    """
    æ ¹æ“š frequencies (å­—è©: æ¬¡æ•¸) ç”¢ç”Ÿæ–‡å­—é›²ä¸¦è¼¸å‡ºç‚ºæª”æ¡ˆã€‚
    frequencies: dict, e.g. {'é—œéµè©A': 10, 'é—œéµè©B': 8, ...}
    """
    if not frequencies:
        return  # æ²’æœ‰ä»»ä½•å­—è©å°±è·³é

    system = platform.system()
    if system == 'Darwin':
        font_path = "/System/Library/Fonts/STHeiti Medium.ttc"
    elif system == 'Windows':
        font_path = r"C:\Windows\Fonts\msjh.ttc"
    else:
        font_path = "/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf"

    wc = WordCloud(
        background_color="white",
        font_path=font_path,
        width=1096,
        height=480,
        margin=5,
        max_words=max_words,
        min_font_size=min_font_size,
        color_func=random_color_func,
        prefer_horizontal=0.9,
    )
    wc.generate_from_frequencies(frequencies)

    os.makedirs(os.path.dirname(save_path), exist_ok=True)
    wc.to_file(save_path)

    return

def news_counter(articles):
    # å°‡è³‡æ–™æ•´ç†æˆæ¯æ—¥æ•¸é‡ dict
    daily_counts = Counter()
    for article in articles:
        if article['date']:
            daily_counts[article['date']] += 1

    # è½‰ç‚ºæ’åºå¾Œçš„ x, y list
    trend_labels = sorted(daily_counts.keys())
    trend_values = [daily_counts[date] for date in trend_labels]
    return trend_labels,trend_values

def generate_prompt(keyword, sentiment_count, top_words, sentiment_by_cat):
    prompt = f"""
    è«‹æ ¹æ“šä»¥ä¸‹è³‡æ–™ï¼Œæ’°å¯«ä¸€ç¯‡åˆ†æå ±å‘Šï¼Œå­—æ•¸ç´„ 300 å­—ï¼Œä»¥æ•¸æ“šåˆ†æå¸«çš„è§’åº¦å»åˆ†æï¼Œèªæ°£å°ˆæ¥­æ¸…æ¥šï¼š
    
    ğŸ” ä¸»é¡Œï¼š{keyword}

    ğŸ“Š æƒ…ç·’æ¯”ä¾‹ï¼š
    æ­£é¢æ–‡ç« æ•¸ï¼š{sentiment_count.get('æ­£é¢', 0)}
    è² é¢æ–‡ç« æ•¸ï¼š{sentiment_count.get('è² é¢', 0)}
    ä¸­ç«‹æ–‡ç« æ•¸ï¼š{sentiment_count.get('ä¸­ç«‹', 0)}

    ğŸ“Œ é¡åˆ¥æƒ…ç·’çµ±è¨ˆæ¦‚æ³ï¼š
    {sentiment_by_cat}

    ğŸ”¥ é«˜é »é—œéµè©(æ¯å€‹è©åœ¨æ–‡ç« å‡ºç¾çš„æ•¸é‡):
    {"ã€".join(top_words.get('all', []))}

    è«‹ç¶œåˆä»¥ä¸Šè³‡è¨Šï¼Œèªªæ˜ç›®å‰ç†±åº¦è¶¨å‹¢èˆ‡ç¤¾ç¾¤é—œæ³¨é‡é»ã€‚
    """
    return prompt

load_dotenv()

def call_LLM(prompt):
    api_key = os.getenv("GOOGLE_API_KEY")
    if not api_key:
        raise ValueError("âš ï¸ GOOGLE_API_KEY æœªè¨­å®šã€‚è«‹ç¢ºèª .env æª”æ¡ˆå­˜åœ¨ä¸”æœ‰è¨­å®šé‡‘é‘°ã€‚")

    genai.configure(api_key=api_key)
    model = genai.GenerativeModel('gemini-1.5-flash')
    response = model.generate_content(prompt)
    return response.text.strip()

def fetch_article_stats(article, headers=None):
    """
    å¾æ–‡ç« å…§é æŠ“å– view_count èˆ‡ share_countã€‚
    å›å‚³ (view_count:int, share_count:int)ã€‚
    """
    url = article.get('news_url')
    if not url:
        return 0, 0
    headers = headers or {"User-Agent": "Mozilla/5.0"}
    try:
        resp = requests.get(url, headers=headers, timeout=5)
        if resp.status_code != 200:
            return 0, 0
        dom = BeautifulSoup(resp.text, 'html.parser')
        views_tag = dom.select_one('span.views')
        view_count = int(re.sub(r'\D', '', views_tag.text)) if views_tag else 0
        share_tag = dom.select_one('button.share-count')
        share_count = int(re.sub(r'\D', '', share_tag.text)) if share_tag else 0
        return view_count, share_count
    except Exception:
        return 0, 0

def compute_hot_score_by_stats(article, weight_share=5):
    """
    è¨ˆç®—ç†±é–€åˆ†æ•¸ï¼šview_count + weight_share * share_count
    """
    views  = article.get('view_count', 0)
    shares = article.get('share_count', 0)
    return views + shares * weight_share

def get_top_hot_articles_by_stats(articles, top_n=10, weight_share=5, fetch_stats=False):
    """
    å–å‡ºå‰ top_n åç†±é–€æ–°èã€‚
    :param articles: List[dict]ï¼Œæ¯ç­† article æœ€å¥½æœ‰ 'news_url'
    :param top_n: è¦å›å‚³çš„ç­†æ•¸
    :param weight_share: åˆ†äº«æ•¸çš„æ¬Šé‡
    :param fetch_stats: æ˜¯å¦å…ˆæŠ“å…§é çµ±è¨ˆ
    :return: sorted List[dict] å‰ top_n ç­†ï¼Œæœƒåœ¨æ¯ç­† article è£¡æ–°å¢ 'hot_score'
    """
    if fetch_stats:
        for art in articles:
            v, s = fetch_article_stats(art)
            art['view_count']  = v
            art['share_count'] = s

    for art in articles:
        art['hot_score'] = compute_hot_score_by_stats(art, weight_share)

    # ä¾ hot_score é™åº
    sorted_list = sorted(articles, key=lambda x: x['hot_score'], reverse=True)
    return sorted_list[:top_n]

# analyzer/utils.py

import os
import platform
import random
from datetime import datetime, timedelta
from collections import Counter, defaultdict

import jieba.analyse
from bs4 import BeautifulSoup
from wordcloud import WordCloud

# ä»¥ä¸‹çœç•¥å…¶ä»– imports â€¦


BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))


def generate_wordcloud(frequencies, save_path, max_words=100, min_font_size=12):
    """
    æ ¹æ“š frequencies (Counter æˆ– dict) ç”¢ç”Ÿæ–‡å­—é›²ä¸¦å„²å­˜åˆ° save_pathã€‚
    """
    # é¸å­—é«”
    system = platform.system()
    if system == 'Darwin':
        font_path = "/System/Library/Fonts/STHeiti Medium.ttc"
    elif system == 'Windows':
        font_path = r"C:\Windows\Fonts\msjh.ttc"
    else:
        font_path = "/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf"

    wc = WordCloud(
        background_color="white",
        font_path=font_path,
        width=800,
        height=450,
        max_words=max_words,
        min_font_size=min_font_size,
        random_state=42
    )
    wc.generate_from_frequencies(frequencies)
    wc.to_file(save_path)


def work(keyword):
    ts = datetime.now().strftime("%Y%m%d_%H%M")

    # 1. æœå°‹èˆ‡æƒ…ç·’åˆ†æ
    articles = analyze_sentiment(search_news(keyword))

    # 2. ç†±åº¦è²¼æ–‡
    hot_articles = get_top_hot_articles_by_stats(articles, top_n=10, weight_share=5, fetch_stats=False)

    # 3. å„ç¨®çµ±è¨ˆ
    sentiment_count = count_sentiment(articles)
    trend_labels, trend_values = news_counter(articles)
    top_word = get_top_words(articles)
    category_stats = sentiment_by_category(articles)

    # 4. æ”¶é›†æ‰€æœ‰ news_tag ä½œç‚ºæ–‡å­—é›²çš„è©å½™
    all_tags = []
    for art in articles:
        all_tags.extend(art.get('news_tag', []))

    # 5. ç”¢ç”Ÿæ–‡å­—é›²
    wordcloud_dir = os.path.join(BASE_DIR, 'static', 'clouds')
    os.makedirs(wordcloud_dir, exist_ok=True)
    filename  = f"{keyword}_{ts}.png"
    save_path = os.path.join(wordcloud_dir, filename)

    if all_tags:
        freqs = dict(Counter(all_tags).most_common(50))
        generate_wordcloud(freqs, save_path, max_words=50, min_font_size=20)
        tag_image = f"clouds/{filename}"
    else:
        tag_image = None

    # 6. å‰ 5 åç›¸é—œé—œéµå­—
    top_keywords = Counter(all_tags).most_common(5)

    # 7. LLM å ±å‘Š
    prompt = generate_prompt(keyword, sentiment_count, top_word, category_stats)
    try:
        report = call_LLM(prompt)
    except Exception as e:
        report = f"âš ï¸ Gemini å›æ‡‰å¤±æ•—ï¼š{e}"

    return {
        'articles':        articles,
        'sentiment_count': sentiment_count,
        'hot_articles':    hot_articles,
        'top_word':        top_word,
        'category_stats':  category_stats,
        'tag_image':       tag_image,
        'trend_labels':    trend_labels,
        'trend_values':    trend_values,
        'top_keywords':    top_keywords,
        'AIreport':        report,
    }

def infer_site_type(source_name):
    """
    æ ¹æ“šä¾†æºåç¨±ç°¡å–®æ¨æ–·ç¶²ç«™é¡å‹ï¼Œ
    ä½ å¯ä»¥æ“´å……é€™å€‹å°ç…§è¡¨ï¼ŒæŠŠå„ç¨®ä¾†æºå°æ‡‰åˆ°ï¼šYTï¼æ–°èç¶²ï¼è«–å£‡ï¼å¾®åšetc.
    """
    if "æ–°èç¶²" in source_name or "ETtoday" in source_name or "è¯åˆæ–°èç¶²" in source_name:
        return "News"
    if source_name.upper().endswith("NEWS") or "TVBS" in source_name:
        return "News"
    # æœªä¾†å†åŠ æ›´å¤šè¦å‰‡
    return "Other"

def compute_source_ranking(articles_by_keyword):
    """
    articles_by_keyword: dict { keyword_str: [article_dict, ...] }
    å›å‚³ä¸€å€‹ DataFrameï¼Œæ¬„ä½ï¼š
      - è¨è«–é¢å‘ (keyword)
      - ç¶²ç«™é¡å‹ (site_type)
      - ä¾†æºåç¨±   (source)
      - è²¼æ–‡æ•¸     (count)
    ä¸¦ä¾ count DESC æ’åºã€‚
    """
    records = []
    for keyword, articles in articles_by_keyword.items():
        for art in articles:
            src = art.get('source', 'Unknown')
            st  = infer_site_type(src)
            records.append({
                'è¨è«–é¢å‘': keyword,
                'ç¶²ç«™é¡å‹': st,
                'ä¾†æºåç¨±': src,
            })
    df = pd.DataFrame(records)
    ranking = (
        df
        .groupby(['è¨è«–é¢å‘', 'ç¶²ç«™é¡å‹', 'ä¾†æºåç¨±'])
        .size()
        .reset_index(name='è²¼æ–‡æ•¸')
        .sort_values('è²¼æ–‡æ•¸', ascending=False)
        .reset_index(drop=True)
    )
    return ranking

def work_with_ranking(keyword):
    result = work(keyword)
    ranking = compute_source_ranking({keyword: result['articles']})
    result['source_ranking'] = ranking
    return result


def infer_site_type(source_name):
    if "æ–°èç¶²" in source_name or "ETtoday" in source_name or "è¯åˆæ–°èç¶²" in source_name:
        return "News"
    if source_name.upper().endswith("NEWS") or "TVBS" in source_name:
        return "News"
    # æ›´å¤šè¦å‰‡...
    return "Other"

def fetch_article_stats(article, headers=None):
    url = article.get('news_url')
    if not url:
        return 0, 0
    headers = headers or {"User-Agent":"Mozilla/5.0"}
    try:
        resp = requests.get(url, headers=headers, timeout=5)
        if resp.status_code != 200:
            return 0, 0
        dom = BeautifulSoup(resp.text, 'html.parser')
        # å‡è¨­ç•™è¨€æ•¸åœ¨ <span class="comments">1234</span>
        comments_tag = dom.select_one('span.comments')
        comments = int(re.sub(r'\D','', comments_tag.text)) if comments_tag else 0
        # å‡è¨­åˆ†äº«æ•¸åœ¨ <button class="share-count">56</button>
        share_tag   = dom.select_one('button.share-count')
        shares      = int(re.sub(r'\D','', share_tag.text))   if share_tag   else 0
        return comments, shares
    except:
        return 0, 0

def compute_hot_score_by_stats(article, weight_share=5):
    comments = article.get('comment_count', 0)
    shares   = article.get('share_count', 0)
    # ä½ ä¹Ÿå¯ä»¥èª¿æ•´ç®—æ³•
    return comments + shares * weight_share

def get_top_hot_articles_by_stats(articles, top_n=10, weight_share=5, fetch_stats=False):
    # é¸è¦ä¸è¦å…ˆæŠ“å…§é ç•™è¨€ï¼åˆ†äº«æ•¸
    if fetch_stats:
        for art in articles:
            c, s = fetch_article_stats(art)
            art['comment_count'] = c
            art['share_count']   = s

    for art in articles:
        art['hot_score'] = compute_hot_score_by_stats(art, weight_share)
        # åŠ ä¸Šã€Œè¨è«–é¢å‘ã€å’Œã€Œç¶²ç«™é¡å‹ã€æ–¹ä¾¿å‰ç«¯é¡¯ç¤ºã€éæ¿¾
        art.setdefault('discussion', art.get('keyword'))  # å‡è¨­ article æ²’æœ‰é€™æ¬„æ™‚ï¼Œç”¨ keyword
        art['site_type'] = infer_site_type(art.get('source',''))
    # å–å‰ top_n
    sorted_list = sorted(articles, key=lambda x: x['hot_score'], reverse=True)
    return sorted_list[:top_n]

import re, requests
from bs4 import BeautifulSoup
from collections import defaultdict
# å·²æœ‰ infer_site_type, parse_date ç­‰å‡½æ•°

def fetch_article_comments(article, max_comments=5):
    """
    å¾æ–‡ç« å…§é æŠ“å‡ºè©•è«–ç¯€é»ï¼Œå›å‚³ list[{
        'timestamp': str,
        'content': str,
        'reaction_count': int
    }]
    """
    comments = []
    url = article.get('news_url')
    if not url:
        return comments

    try:
        resp = requests.get(url, headers={"User-Agent":"Mozilla/5.0"}, timeout=5)
        if resp.status_code != 200:
            return comments
        dom = BeautifulSoup(resp.text, 'html.parser')
        # ä¸‹é¢ selector ä¾å¯¦éš›ç¶²ç«™èª¿æ•´ï¼š
        nodes = dom.select('.comment-item')[:max_comments]
        for nd in nodes:
            ts = nd.select_one('.comment-time')
            txt = nd.select_one('.comment-text')
            rc = nd.select_one('.comment-reactions')
            timestamp      = ts.get_text(strip=True) if ts else ''
            content        = txt.get_text(strip=True) if txt else ''
            reaction_count = int(re.sub(r'\D','', rc.get_text())) if rc else 0
            comments.append({
                'timestamp': timestamp,
                'content': content,
                'reaction_count': reaction_count
            })
    except Exception:
        pass

    return comments

def get_top_hot_comments_by_reactions(articles, top_n=10, per_article=5):
    """
    æŠ“æ‰€æœ‰æ–‡ç« çš„å‰ per_article æ¢è©•è«–ï¼Œå½™æ•´å¾Œä¾ reaction_count æ’åºå– top_n
    å›å‚³ list[{
      'region':         str,
      'source_category':str,
      'timestamp':      str,
      'content':        str,
      'reaction_count': int,
    }]
    """
    all_comments = []
    for art in articles:
        region = art.get('category', art.get('source',''))
        site_cat = infer_site_type(art.get('source',''))
        for c in fetch_article_comments(art, max_comments=per_article):
            all_comments.append({
                'region':          region,
                'source_category': site_cat,
                'timestamp':       c['timestamp'],
                'content':         c['content'],
                'reaction_count':  c['reaction_count'],
            })
    # å…¨éƒ¨æ’åºä¸¦å–å‰ top_n
    sorted_comments = sorted(all_comments,
                             key=lambda x: x['reaction_count'],
                             reverse=True)
    return sorted_comments[:top_n]


# source venv/bin/activate
# python manage.py runserver