from datetime import datetime
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from dateutil import parser
from dateutil import tz
import time
import json
import random
import re
import jieba.analyse


# æª¢æŸ¥è²¼æ–‡ï¼šè‡³å°‘æœ‰60ï¼…ä¸­æ–‡ 
def is_mostly_chinese(text, threshold=0.6):
    chinese_chars = re.findall(r"[\u4e00-\u9fff]", text)
    return len(chinese_chars) / max(len(text), 1) >= threshold


# æª¢æŸ¥è²¼æ–‡ï¼šè‡³å°‘30å€‹å­— ä¸­æ–‡å­— 
def count_chinese_chars(text):
    return len(re.findall(r"[\u4e00-\u9fff]", text))

# ç”¢ç”Ÿä¸»é¡Œ titleï¼ˆä½¿ç”¨ jiebaï¼‰
def generate_title_with_keywords(text, topk=3):
    """
    å¾ä¸€æ®µæ–‡å­—ä¸­æŠ½å–é—œéµè©ä½œç‚ºä¸»é¡Œã€‚
    Args:
        text (str): è²¼æ–‡å…§å®¹
        topk (int): é¸å¹¾å€‹é—œéµè©çµ„æˆä¸»é¡Œ
    Returns:
        str: ç”±é—œéµè©çµ„æˆçš„ä¸»é¡Œï¼Œä¾‹å¦‚ã€Œæ²–ç¹©ï½œæ¨è–¦ï½œæµ·ã€
    """
    keywords = jieba.analyse.extract_tags(text, topK=topk)
    return "ï½œ".join(keywords) if keywords else "ç„¡æ³•ç”¢ç”Ÿä¸»é¡Œ"

# ä¸»ç¨‹å¼
def scrape_threads_posts():
    start_time = datetime.now()
    options = webdriver.ChromeOptions()
    options.add_argument('--disable-gpu')
    options.add_argument('--no-sandbox')
    options.add_argument('--disable-dev-shm-usage')
    options.add_argument("user-agent=Mozilla/5.0")

    driver = webdriver.Chrome(options=options)
    driver.get("https://www.threads.com/")
    WebDriverWait(driver, 20).until(
        EC.presence_of_element_located((By.XPATH, "//div[@data-pressable-container='true']"))
    )

    # ----------- è¨­å®šåƒæ•¸ ------------
    MAX_TARGET = 5
    MAX_SCROLLS = 40
    MAX_NO_NEW_SCROLLS = 2

    all_posts_data = []
    post_index = 0
    scroll_round = 0
    no_new_scrolls = 0

    while len(all_posts_data) < MAX_TARGET and scroll_round < MAX_SCROLLS:
        scroll_round += 1
        print(f"\nğŸ”„ æ»¾å‹•ç¬¬ {scroll_round} æ¬¡...")

        prev_posts_len = len(driver.find_elements(By.XPATH, "//div[@data-pressable-container='true']"))
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        time.sleep(random.uniform(2, 4))

        posts = driver.find_elements(By.XPATH, "//div[@data-pressable-container='true']")

        if len(posts) == prev_posts_len:
            no_new_scrolls += 1
            print("âš ï¸ æ­¤è¼ªæœªè¼‰å…¥æ–°è²¼æ–‡")
            if no_new_scrolls >= MAX_NO_NEW_SCROLLS:
                print("ğŸ›‘ å·²é€£çºŒå¤šæ¬¡ç„¡æ–°å¢è²¼æ–‡ï¼ŒçµæŸã€‚")
                break
        else:
            no_new_scrolls = 0

        while post_index < len(posts):
            post = posts[post_index]
            post_index += 1
            print(f"\nğŸ“ æ­£åœ¨è™•ç†ç¬¬ {post_index} ç­†è²¼æ–‡")

            try:
                author = post.find_element(By.XPATH, ".//span[contains(@class, 'x1lliihq') and ancestor::a[contains(@href, '/@')]]").text.strip()
            except:
                author = "N/A"

            try:
                link_elem = post.find_element(By.XPATH, ".//a[@role='link'][time]")
                href = link_elem.get_attribute("href")
                post_link = "https://www.threads.com" + href if href.startswith("/") else href
            except:
                post_link = "N/A"

            try:
                time_str = post.find_element(By.XPATH, ".//time").get_attribute("datetime")
                utc_time = parser.parse(time_str)
                taipei_time = utc_time.astimezone(tz.gettz("Asia/Taipei"))
                post_time = taipei_time.strftime("%Y-%m-%d %H:%M:%S")
            except:
                post_time = "N/A"

            try:
                spans = post.find_elements(By.XPATH, ".//div[contains(@class, 'x1a6qonq')]//span[contains(@class, 'x1lliihq')]//span")
                parts = [s.text.strip() for s in spans if s.text.strip()]
                post_text = "\n".join(sorted(set(parts), key=parts.index))
                post_title = generate_title_with_keywords(post_text)

            except:
                post_text = "N/A"

            # has_image = bool(post.find_elements(By.XPATH, ".//img"))
            # has_video = bool(post.find_elements(By.XPATH, ".//video"))

            chinese_count = count_chinese_chars(post_text)
            if not is_mostly_chinese(post_text):
                print("âŒ éä¸­æ–‡ä¸»é«”ï¼Œè·³éã€‚")
                continue
            if chinese_count < 30:
                print("âŒ ä¸­æ–‡å­— < 30ï¼Œè·³éã€‚")
                continue

            comments = []
            if post_link != "N/A":
                try:
                    driver.execute_script("window.open(arguments[0]);", post_link)
                    time.sleep(3)
                    new_tab = [w for w in driver.window_handles if w != driver.current_window_handle][-1]
                    driver.switch_to.window(new_tab)
                    WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.TAG_NAME, "body")))

                    for _ in range(3):
                        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
                        time.sleep(2)

                    wrapper = driver.find_element(
                        By.XPATH,
                        "//div[contains(@class, 'xb57i2i') and contains(@class, 'x1q594ok') and contains(@class, 'x5lxg6s') and contains(@class, 'x78zum5') and contains(@class, 'xdt5ytf') and contains(@class, 'x1ja2u2z') and contains(@class, 'x1pq812k') and contains(@class, 'x1rohswg') and contains(@class, 'xfk6m8') and contains(@class, 'x1yqm8si') and contains(@class, 'xjx87ck') and contains(@class, 'xx8ngbg') and contains(@class, 'xwo3gff') and contains(@class, 'x1n2onr6') and contains(@class, 'x1oyok0e') and contains(@class, 'x1e4zzel') and contains(@class, 'x1plvlek') and contains(@class, 'xryxfnj')]"
                    )

                    blocks = wrapper.find_elements(By.XPATH, ".//div[contains(@class, 'x1a6qonq')]")[1:]

                    for block in blocks:
                        spans = block.find_elements(By.XPATH, ".//span[@dir='auto']/span")
                        text = "\n".join([s.text.strip() for s in spans if s.text.strip()])
                        if text:
                            comments.append(text)
                        if len(comments) >= 10:
                            break

                    print(f"ğŸ’¬ æ“·å–ç•™è¨€æ•¸ï¼š{len(comments)}")
                    driver.close()
                    driver.switch_to.window(driver.window_handles[0])
                except Exception as e:
                    print("ç•™è¨€æŠ“å–éŒ¯èª¤:", e)
                    try:
                        driver.close()
                        driver.switch_to.window(driver.window_handles[0])
                    except:
                        pass

            all_posts_data.append({
                "author": author,
                "link": post_link,
                "content": post_text,
                "title": post_title,
                # "has_image": has_image,
                # "has_video": has_video,
                "post_time": post_time,
                "comments": comments
            })
            print(f"âœ… æ”¶éŒ„ç¬¬ {len(all_posts_data)} ç¯‡è²¼æ–‡")

            if len(all_posts_data) >= MAX_TARGET:
                break
            time.sleep(random.uniform(1, 2))

    # å„²å­˜ JSON
    end_time = datetime.now()
    elapsed = end_time - start_time

    output = {
        "summary": {
            "total_posts": len(all_posts_data),
            "elapsed_time": f"{elapsed.seconds // 60} åˆ† {elapsed.seconds % 60} ç§’"
        },
        "posts": all_posts_data
    }

    with open("threads_posts.json", "w", encoding="utf-8") as f:
        json.dump(output, f, ensure_ascii=False, indent=2)

    print(f"\nğŸ“ å·²å„²å­˜ threads_posts.jsonï¼ˆå…± {len(all_posts_data)} ç¯‡ï¼‰")
    print(f"âŒ› ç¸½è€—æ™‚ï¼š{elapsed.seconds // 60} åˆ† {elapsed.seconds % 60} ç§’")
    driver.quit()

if __name__ == "__main__":
    scrape_threads_posts()
