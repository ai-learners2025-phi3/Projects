from datetime import datetime
import time, json, random
from dateutil import parser, tz
import re

from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException

import jieba.analyse


# --- ç™»å…¥ Threads (Instagram)
def login_to_threads(driver):
    IG_USERNAME = "leafwann_"
    IG_PASSWORD = "threads2025"

    driver.get("https://www.threads.net/login")
    try:
        login_button = WebDriverWait(driver, 15).until(
            EC.presence_of_element_located((By.XPATH, "//span[text()='ä½¿ç”¨ Instagram å¸³è™Ÿç¹¼çºŒ']"))
        )
        login_button.click()
    except:
        print("âŒ æ‰¾ä¸åˆ°ç™»å…¥æŒ‰éˆ•")
        return False

    try:
        WebDriverWait(driver, 15).until(EC.presence_of_element_located((By.NAME, "username")))
        driver.find_element(By.NAME, "username").send_keys(IG_USERNAME)
        driver.find_element(By.NAME, "password").send_keys(IG_PASSWORD)
        driver.find_element(By.NAME, "password").send_keys(Keys.ENTER)
        WebDriverWait(driver, 20).until(
            EC.presence_of_element_located((By.XPATH, "//div[@data-pressable-container='true']"))
        )
        return True
    except Exception as e:
        print("âŒ ç™»å…¥å¤±æ•—ï¼š", e)
        return False

# --- æ“·å–ç•™è¨€ï¼ˆé¿é–‹ä¸»æ–‡ã€åªæŠ“æŒ‡å®šç•™è¨€å®¹å™¨ï¼‰
def scrape_comments_from_post_page(driver):
    comments = []
    try:
        wrapper = driver.find_element(By.XPATH, "//div[contains(@class,'xb57i2i') and contains(@class,'x1q594ok')]")
        comment_blocks = wrapper.find_elements(By.XPATH, ".//div[contains(@class, 'x1a6qonq')]")[1:]
        for block in comment_blocks:
            spans = block.find_elements(By.XPATH, ".//span[@dir='auto']/span")
            text = "\n".join([s.text.strip() for s in spans if s.text.strip()])
            if text:
                comments.append(text)
            if len(comments) >= 10:
                break
    except Exception as e:
        print("ç•™è¨€æ“·å–éŒ¯èª¤ï¼š", e)
    return comments


# ç”¢ç”Ÿä¸»é¡Œ titleï¼ˆä½¿ç”¨ jiebaï¼‰
def generate_title_with_keywords(text, topk=3):
    """
    å¾ä¸€æ®µæ–‡å­—ä¸­æŠ½å–é—œéµè©ä½œç‚ºä¸»é¡Œã€‚
    Args:
        text (str): è²¼æ–‡å…§å®¹
        topk (int): é¸å¹¾å€‹é—œéµè©çµ„æˆä¸»é¡Œ
    Returns:
        str: ç”±é—œéµè©çµ„æˆçš„ä¸»é¡Œï¼Œä¾‹å¦‚ã€Œæ²–ç¹©ï½œæ¨è–¦ï½œæµ·ã€
    """
    keywords = jieba.analyse.extract_tags(text, topK=topk)
    return "ï½œ".join(keywords) if keywords else "ç„¡æ³•ç”¢ç”Ÿä¸»é¡Œ"


# --- Threads è²¼æ–‡çˆ¬å–ä¸»å‡½æ•¸ï¼ˆé—œéµå­—æ¨¡å¼ï¼‰
def scrape_threads_by_keyword():
    keyword_to_search = input("è«‹è¼¸å…¥è¦æœå°‹çš„é—œéµå­—ï¼š")

    MAX_TARGET = 50
    MAX_SCROLLS = 40
    MAX_NO_NEW_SCROLLS = 2

    start_time = datetime.now()
    options = webdriver.ChromeOptions()
    options.add_argument("--disable-gpu")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("user-agent=Mozilla/5.0")

    driver = webdriver.Chrome(options=options)

    try:
        if not login_to_threads(driver):
            return

        driver.get("https://www.threads.com/search?hl=zh-tw")
        search_input = WebDriverWait(driver, 15).until(
            EC.presence_of_element_located((By.XPATH, "//input[@type='search' and @placeholder='æœå°‹']"))
        )
        search_input.send_keys(keyword_to_search)
        search_input.send_keys(Keys.ENTER)

        WebDriverWait(driver, 15).until(
            EC.presence_of_element_located((By.XPATH, "//div[@data-pressable-container='true']"))
        )

        all_posts_data = []
        post_index = 0
        scroll_round = 0
        no_new_scrolls = 0
        last_post_count = 0
        main_window_handle = driver.current_window_handle

        while len(all_posts_data) < MAX_TARGET and scroll_round < MAX_SCROLLS:
            scroll_round += 1
            driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(random.uniform(2, 4))

            posts = driver.find_elements(By.XPATH, "//div[@data-pressable-container='true']")
            if len(posts) == last_post_count:
                no_new_scrolls += 1
                if no_new_scrolls >= MAX_NO_NEW_SCROLLS:
                    print("âš ï¸ å¤šæ¬¡æ»¾å‹•ç„¡æ–°å…§å®¹ï¼Œåœæ­¢ã€‚")
                    break
            else:
                no_new_scrolls = 0
                last_post_count = len(posts)

            while post_index < len(posts) and len(all_posts_data) < MAX_TARGET:
                post = posts[post_index]
                post_index += 1
                print(f"\nğŸ“ æ­£åœ¨è™•ç†ç¬¬ {post_index} ç¯‡è²¼æ–‡")

                try:
                    author = post.find_element(By.XPATH, ".//span[contains(@class,'x1lliihq') and ancestor::a[contains(@href, '/@')]]").text.strip()
                except: author = "N/A"

                try:
                    permalink = post.find_element(By.XPATH, ".//a[@role='link'][time]").get_attribute("href")
                    post_link = "https://www.threads.com" + permalink if permalink.startswith("/") else permalink
                except: post_link = "N/A"

                try:
                    raw_time = post.find_element(By.XPATH, ".//time").get_attribute("datetime")
                    taipei_time = parser.parse(raw_time).astimezone(tz.gettz("Asia/Taipei"))
                    post_time = taipei_time.strftime("%Y-%m-%d %H:%M:%S")
                except: post_time = "N/A"

                try:
                    spans = post.find_elements(By.XPATH, ".//div[contains(@class,'x1a6qonq') and contains(@class,'x6ikm8r')]//span[contains(@class,'x1lliihq')]//span")
                    parts = [s.text.strip() for s in spans if s.text.strip()]
                    post_text = "\n".join(sorted(set(parts), key=parts.index))
                    post_title = generate_title_with_keywords(post_text)

                except: post_text = "N/A"

                # has_image = bool(post.find_elements(By.XPATH, ".//img"))
                # has_video = bool(post.find_elements(By.XPATH, ".//video"))

                # æŠ“ç•™è¨€
                comments_data = []
                if post_link != "N/A":
                    try:
                        driver.execute_script("window.open(arguments[0]);", post_link)
                        time.sleep(2)
                        new_tab = [w for w in driver.window_handles if w != main_window_handle][-1]
                        driver.switch_to.window(new_tab)
                        WebDriverWait(driver, 10).until(
                            EC.presence_of_element_located((By.TAG_NAME, "body"))
                        )
                        comments_data = scrape_comments_from_post_page(driver)
                        driver.close()
                        driver.switch_to.window(main_window_handle)
                    except Exception as e:
                        print("ç•™è¨€æŠ“å–éŒ¯èª¤ï¼š", e)
                        if len(driver.window_handles) > 1:
                            driver.close()
                        driver.switch_to.window(main_window_handle)

                all_posts_data.append({
                    "author": author,
                    "link": post_link,
                    "post_time": post_time,
                    "title": post_title,
                    "content": post_text,
                    # "has_image": has_image,
                    # "has_video": has_video,
                    "comments": comments_data
                })

                print(f"âœ… æ”¶éŒ„ç¬¬ {len(all_posts_data)} ç¯‡è²¼æ–‡")
                time.sleep(random.uniform(1, 2))

        elapsed = datetime.now() - start_time
        result = {
            "summary": {
                "total_posts": len(all_posts_data),
                "elapsed_time": f"{elapsed.seconds // 60} åˆ† {elapsed.seconds % 60} ç§’"
            },
            "posts": all_posts_data
        }

        filename = f"threads_posts_{keyword_to_search}.json"
        with open(filename, "w", encoding="utf-8") as f:
            json.dump(result, f, ensure_ascii=False, indent=2)

        print(f"\nğŸ“ å·²å„²å­˜ç‚º {filename}")
        print(f"ğŸ“Š å…±æŠ“å– {len(all_posts_data)} ç­†ï¼Œè€—æ™‚ {elapsed.seconds // 60} åˆ† {elapsed.seconds % 60} ç§’")

    finally:
        print("ğŸ§¹ é—œé–‰ç€è¦½å™¨...")
        driver.quit()
        print("âœ… çµæŸ")

# --- åŸ·è¡Œ
if __name__ == "__main__":
    scrape_threads_by_keyword()
